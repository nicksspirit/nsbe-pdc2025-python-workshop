{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chapter 1: Exploratory Data Analysis of the U.S. Job Market\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Load and explore the OEWS (Occupational Employment and Wage Statistics) dataset\n",
    "- Create meaningful visualizations to understand job market trends\n",
    "- Identify target occupations and states for deeper analysis\n",
    "- Use data-driven insights to guide subsequent scraping activities\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to the OEWS Dataset\n",
    "\n",
    "> **Instructor Cue:** Start by opening the BLS website (https://www.bls.gov/oes/) in your browser. Show the attendees where this data comes from and emphasize its credibility as a government source. Mention that this dataset is updated annually and covers over 800 occupations across all states.\n",
    "\n",
    "The U.S. Bureau of Labor Statistics (BLS) Occupational Employment and Wage Statistics (OEWS) program provides employment and wage estimates for hundreds of occupations. This dataset serves as our foundation for understanding the national job landscape before we dive into targeted data collection.\n",
    "\n",
    "### Key Dataset Columns\n",
    "\n",
    "Before we begin our analysis, let's understand the structure of our data:\n",
    "\n",
    "- `occ_code`: Occupation classification code\n",
    "- `occ_title`: Occupation title (e.g., \"Data Scientists\")\n",
    "- `o_group`: Occupation group level (detailed, minor, major)\n",
    "- `tot_emp`: Total employment\n",
    "- `jobs_1000`: Employment per 1,000 jobs\n",
    "- `a_mean`: Annual mean wage\n",
    "- `h_mean`: Hourly mean wage\n",
    "- `state`: State abbreviation\n",
    "\n",
    "> **Instructor Cue:** Ask the audience: \"What questions might we want to ask about this data? What patterns would be interesting to explore?\" Write their responses on a whiteboard to refer back to during the analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Our Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and loading our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)  # Set default matplotlib figure size\n",
    "\n",
    "# --- Caching Setup ---\n",
    "# Cache requests to avoid re-downloading the same file from the web\n",
    "# The cache will be stored in '.requests_cache' and expire after 1 day.\n",
    "requests_cache.install_cache(\".requests_cache\", expire_after=timedelta(days=1))\n",
    "\n",
    "BASE_DIR = Path().cwd()\n",
    "\n",
    "print(f\"{BASE_DIR=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Emphasize that we're using seaborn with matplotlib's object-oriented API throughout this workshop. This is a best practice for creating publication-quality visualizations. Also highlight that we'll first download the data directly from the BLS website as an example of programmatic data acquisition.\n",
    "\n",
    "### Downloading Data Directly from the Source\n",
    "\n",
    "Before we analyze the data, let's practice an important data acquisition skill: downloading data programmatically from its source. The Bureau of Labor Statistics (BLS) provides the OEWS dataset as a downloadable ZIP file.\n",
    "\n",
    "> **Instructor Cue:** Explain that in real-world scenarios data scientists often need to fetch data from original sources. This ensures we're working with the most current information and builds important data collection skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data source and local storage paths\n",
    "ZIP_URL = \"https://www.bls.gov/oes/special-requests/oesm24ma.zip\"\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "# PROTIP: The pathlib.Path object can be used work with URLs to extract components\n",
    "zip_filename = Path(ZIP_URL).name\n",
    "zip_path = DATA_DIR / zip_filename\n",
    "\n",
    "# Make sure the data directory exists\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Requesting OEWS dataset from {ZIP_URL}...\")\n",
    "\n",
    "# Set headers to mimic a web browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://www.bls.gov/oes/special-requests/\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Download the file with a streaming approach to handle large files\n",
    "    response = requests.get(ZIP_URL, headers=headers, stream=True, timeout=30)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Check if the response was from the cache\n",
    "    if getattr(response, \"from_cache\", False):\n",
    "        print(\"Data was served from local cache.\")\n",
    "    else:\n",
    "        print(\"Data was fetched from the network.\")\n",
    "\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    print(f\"Data saved to {zip_path}\")\n",
    "except requests.HTTPError as e:\n",
    "    print(f\"HTTP error: {e}\")\n",
    "    print(\n",
    "        f\"If you still get a 403 error, you may need to download manually from:\\n{ZIP_URL}\\nand save it to: {zip_path}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Download failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Let's unzip our file and see what we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_path, extract_to):\n",
    "    \"\"\"Extract all contents of a zip file to the specified directory\"\"\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    print(f\"Successfully unzipped {zip_path}\")\n",
    "\n",
    "\n",
    "# Only unzip if the zip file exists\n",
    "if zip_path.exists():\n",
    "    unzip_file(zip_path, DATA_DIR)\n",
    "\n",
    "    # List extracted files to identify the one we need\n",
    "    print(\"\\nExtracted files:\")\n",
    "    zip_contents = zipfile.ZipFile(zip_path).namelist()\n",
    "\n",
    "    for i, file in enumerate(zip_contents[:10]):  # Show first 10 files\n",
    "        if file.endswith(\"/\"):  # Directory\n",
    "            print(f\"- {file}\")\n",
    "        else:  # File\n",
    "            print(f\"\\t- {file}\")\n",
    "\n",
    "    if len(zip_contents) > 10:\n",
    "        print(f\"...and {len(zip_contents) - 10} more files\")\n",
    "else:\n",
    "    print(\"Zip file not available. Skipping extraction step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Processing the BLS Excel Data\n",
    "\n",
    "Looking at the extracted files, we can see that the BLS OEWS data comes as multiple Excel files within the ZIP archive. For our analysis, we'll use the `MSA_M2024_dl.xlsx` file which contains metropolitan statistical area data.\n",
    "\n",
    "> **Instructor Cue:** Explain that Excel files often contain multiple sheets and raw data that needs cleaning. Walk through how we examine the file structure before deciding on our processing approach. Highlight that this transformation from raw data to analysis-ready format is a key data science skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process the Excel file into a dataframe\n",
    "\n",
    "excel_path = DATA_DIR / \"oesm24ma\" / \"MSA_M2024_dl.xlsx\"\n",
    "\n",
    "if excel_path.exists():\n",
    "    # Load the Excel file - using sheet_name=None to get all sheets as a dictionary\n",
    "    excel_data = pd.read_excel(excel_path, sheet_name=None)\n",
    "\n",
    "    # Print available sheets to help us understand the file structure\n",
    "    print(f\"Available sheets in the Excel file: {list(excel_data.keys())}\")\n",
    "\n",
    "    # Let's use the main sheet (assuming it's the first one)\n",
    "    main_sheet = list(excel_data.keys())[0]\n",
    "    oews_df = excel_data[main_sheet]\n",
    "\n",
    "    # Display basic information about the dataset\n",
    "    print(f\"\\nDataset shape: {oews_df.shape}\")\n",
    "\n",
    "    # Preview the first few rows\n",
    "    display(oews_df.head())\n",
    "\n",
    "# Rename our dataframe to df for consistency with the rest of the notebook\n",
    "df = oews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our dataframe more thoroughly\n",
    "print(\"=== Data Information ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get summary statistics for numeric columns\n",
    "print(\"=== Summary Statistics ===\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Quality Check\n",
    "\n",
    "> **Instructor Cue:** Walk through the output together. Point out any surprising values or data types. Ask: \"What do you notice about the data? Any immediate red flags or interesting patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isna().sum()\n",
    "display(\n",
    "    missing_data[missing_data > 0]\n",
    "    .to_frame(\"n_missing_values\")\n",
    "    .style.set_caption(\"Missing Values by Column\")\n",
    ")\n",
    "\n",
    "# Check unique values in key categorical columns\n",
    "print(f\"Number of states: {df['PRIM_STATE'].nunique()}\")\n",
    "print(f\"Sample states: {df['PRIM_STATE'].unique()[:10]}\")\n",
    "print(\"\\nColumn names for reference:\")\n",
    "pp.pprint(list(df.columns), compact=True, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## EDA Visualizations\n",
    "\n",
    "Before we create visualizations, let's clean our data to focus only on complete records and detailed occupations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric (handling any non-numeric values)\n",
    "cols_to_convert = [\n",
    "    \"H_MEAN\",\n",
    "    \"A_MEAN\",\n",
    "    \"TOT_EMP\",\n",
    "    \"JOBS_1000\",\n",
    "    \"H_PCT25\",\n",
    "    \"H_MEDIAN\",\n",
    "    \"H_PCT75\",\n",
    "    \"A_PCT25\",\n",
    "    \"A_MEDIAN\",\n",
    "    \"A_PCT75\",\n",
    "]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Clean the data for our analysis\n",
    "# Remove rows with missing wage data and focus on detailed occupations\n",
    "df_clean = df[\n",
    "    (df[\"O_GROUP\"] == \"detailed\")\n",
    "    & (df[\"H_MEAN\"].notna())\n",
    "    & (df[\"A_MEAN\"].notna())\n",
    "    & (df[\"TOT_EMP\"].notna())\n",
    "    & (df[\"JOBS_1000\"].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows with missing data\")\n",
    "\n",
    "# Verify the data types after cleaning\n",
    "display(\n",
    "    df_clean[cols_to_convert]\n",
    "    .dtypes.to_frame(\"dtypes\")\n",
    "    .style.set_caption(\"Data Types After Cleaning\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Visualization 1: Top-Paying Occupations by State\n",
    "\n",
    "Let's compare the highest-paying occupations between Oregon and Louisiana to understand regional differences:\n",
    "\n",
    "> **Instructor Cue:** Ask the class: \"Why might we choose Oregon and Louisiana for comparison? What economic differences might we expect?\" Encourage discussion about tech hubs vs. traditional industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wage_comparison_chart(df, states, top_n=20):\n",
    "    \"\"\"\n",
    "    Create a bar chart comparing top-paying occupations between states.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned OEWS dataframe\n",
    "        states: List of state abbreviations to compare\n",
    "        top_n: Number of top occupations to display\n",
    "    \"\"\"\n",
    "    # Filter for target states\n",
    "    state_data = df[df[\"PRIM_STATE\"].isin(states)].copy()\n",
    "\n",
    "    # Get top paying occupations for each state\n",
    "    top_occupations = []\n",
    "    for state in states:\n",
    "        state_subset = state_data[state_data[\"PRIM_STATE\"] == state]\n",
    "        top_state = state_subset.nlargest(top_n, \"H_MEAN\")\n",
    "        top_state[\"rank\"] = range(1, len(top_state) + 1)\n",
    "        top_occupations.append(top_state)\n",
    "\n",
    "    combined_data = pd.concat(top_occupations)\n",
    "\n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "    # Create bar plot\n",
    "    sns.barplot(\n",
    "        data=combined_data,\n",
    "        y=\"OCC_TITLE\",\n",
    "        x=\"H_MEAN\",\n",
    "        hue=\"PRIM_STATE\",\n",
    "        ax=ax,\n",
    "        palette=\"viridis\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Top {top_n} Highest-Paying Occupations: {' vs '.join(states)}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Hourly Mean Wage ($)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Occupation Title\", fontsize=12)\n",
    "\n",
    "    # Improve readability\n",
    "    ax.tick_params(axis=\"y\", labelsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, combined_data\n",
    "\n",
    "\n",
    "# Execute the comparison\n",
    "states_to_compare = [\"OR\", \"LA\"]\n",
    "fig, wage_data = create_wage_comparison_chart(df_clean, states_to_compare)\n",
    "plt.show()\n",
    "\n",
    "# Display top 5 for each state\n",
    "for state in states_to_compare:\n",
    "    state_top = wage_data[wage_data[\"PRIM_STATE\"] == state].head()\n",
    "    print(f\"\\nTop 5 occupations in {state}:\")\n",
    "    print(state_top[[\"OCC_TITLE\", \"H_MEAN\", \"TOT_EMP\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Visualization 2: Employment vs. Salary Relationship\n",
    "\n",
    "Now let's examine how employment levels relate to salary across different states:\n",
    "\n",
    "> **Instructor Cue:** This visualization will help us understand the relationship between job availability and compensation levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "\n",
    "\n",
    "def create_employment_salary_scatter(df, sample_states=None, include_other_states=False):\n",
    "    \"\"\"\n",
    "    Create a scatter plot showing the relationship between employment\n",
    "    density and annual salary by state.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned OEWS dataframe\n",
    "        sample_states: List of states to highlight (optional)\n",
    "    \"\"\"\n",
    "    # Filter for states with sufficient data\n",
    "    state_counts = df[\"PRIM_STATE\"].value_counts()\n",
    "    states_with_data = state_counts[state_counts >= 50].index\n",
    "\n",
    "    plot_data = df[df[\"PRIM_STATE\"].isin(states_with_data)].copy()\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if sample_states:\n",
    "        # Highlight specific states\n",
    "        for state in sample_states:\n",
    "            state_data = plot_data[plot_data[\"PRIM_STATE\"] == state]\n",
    "            ax.scatter(\n",
    "                state_data[\"JOBS_1000\"],\n",
    "                state_data[\"A_MEAN\"],\n",
    "                label=state,\n",
    "                alpha=0.7,\n",
    "                s=60,\n",
    "            )\n",
    "\n",
    "        if include_other_states:\n",
    "            # Plot other states in grey\n",
    "            other_states = plot_data[~plot_data[\"PRIM_STATE\"].isin(sample_states)]\n",
    "            ax.scatter(\n",
    "                other_states[\"JOBS_1000\"],\n",
    "                other_states[\"A_MEAN\"],\n",
    "                color=\"lightgrey\",\n",
    "                alpha=0.3,\n",
    "                s=20,\n",
    "                label=\"Other States\",\n",
    "            )\n",
    "    else:\n",
    "        # Color by state (too many for legend)\n",
    "        sns.scatterplot(\n",
    "            data=plot_data,\n",
    "            x=\"JOBS_1000\",\n",
    "            y=\"A_MEAN\",\n",
    "            hue=\"PRIM_STATE\",\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Employment Density vs. Annual Salary by State\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Employment per 1,000 Jobs\", fontsize=12)\n",
    "    ax.set_ylabel(\"Annual Mean Wage ($)\", fontsize=12)\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "    if sample_states:\n",
    "        ax.legend()\n",
    "\n",
    "    # # Fit a trend line using numpy.polynomial.Polynomial\n",
    "    # # Remove NaNs for fitting\n",
    "    # x_fit = plot_data[\"JOBS_1000\"].dropna()\n",
    "    # y_fit = plot_data[\"A_MEAN\"].dropna()\n",
    "\n",
    "    # # Fit a 1st degree polynomial (linear trend)\n",
    "    # coefs = np.polynomial.Polynomial.fit(x_fit, y_fit, 1).convert().coef\n",
    "    # p = np.polynomial.Polynomial(coefs)\n",
    "    # ax.plot(x_fit, p(x_fit), \"r--\", alpha=0.8, linewidth=2, label=\"Trend Line\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, plot_data\n",
    "\n",
    "\n",
    "# Create the scatter plot highlighting our target states\n",
    "fig, scatter_data = create_employment_salary_scatter(df_clean, [\"OR\", \"LA\"])\n",
    "plt.show()\n",
    "\n",
    "# Analyze the correlation\n",
    "correlation = scatter_data[\"JOBS_1000\"].corr(scatter_data[\"A_MEAN\"])\n",
    "print(f\"\\nCorrelation between employment density and salary: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Visualization 3: State Employment Heatmap\n",
    "\n",
    "Finally, let's create a heatmap to visualize which occupations are most prevalent in different states:\n",
    "\n",
    "> **Instructor Cue:** This visualization will help us identify patterns of occupational concentration across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_heatmap(df, top_occupations=15, top_states=10):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing total employment by occupation and state.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned OEWS dataframe\n",
    "        top_occupations: Number of top occupations to include\n",
    "        top_states: Number of top states to include\n",
    "    \"\"\"\n",
    "    # Find the most prevalent occupations nationally\n",
    "    national_employment = df.groupby(\"OCC_TITLE\")[\"TOT_EMP\"].sum().sort_values(ascending=False)\n",
    "    top_occ_list = national_employment.head(top_occupations).index\n",
    "\n",
    "    # Find states with highest total employment\n",
    "    state_employment = df.groupby(\"PRIM_STATE\")[\"TOT_EMP\"].sum().sort_values(ascending=False)\n",
    "    top_state_list = state_employment.head(top_states).index\n",
    "\n",
    "    # Filter data\n",
    "    heatmap_data = df[\n",
    "        (df[\"OCC_TITLE\"].isin(top_occ_list)) & (df[\"PRIM_STATE\"].isin(top_state_list))\n",
    "    ].copy()\n",
    "\n",
    "    # Create pivot table\n",
    "    pivot_table = heatmap_data.pivot_table(\n",
    "        index=\"OCC_TITLE\", columns=\"PRIM_STATE\", values=\"TOT_EMP\", fill_value=0\n",
    "    )\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    sns.heatmap(\n",
    "        pivot_table,\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": \"Total Employment\"},\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Employment Heatmap: Top {top_occupations} Occupations in Top {top_states} States\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"State\", fontsize=12)\n",
    "    ax.set_ylabel(\"Occupation Title\", fontsize=12)\n",
    "\n",
    "    # Improve label readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, pivot_table\n",
    "\n",
    "\n",
    "# Create the heatmap\n",
    "fig, employment_heatmap = create_occupation_heatmap(df_clean, top_occupations=20)\n",
    "plt.show()\n",
    "\n",
    "# Show some interesting insights\n",
    "if \"Data Scientists\" in employment_heatmap.index:\n",
    "    print(\"\\nStates with highest employment in Data Science roles:\")\n",
    "\n",
    "    data_sci_employment = employment_heatmap.loc[\"Data Scientists\"].sort_values(ascending=False)\n",
    "    print(data_sci_employment.head().to_string())\n",
    "else:\n",
    "    print(\"Data Scientists not in top occupations nationally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_target_occupations(df, criteria=\"high_wage_tech\"):\n",
    "    \"\"\"\n",
    "    Identify target occupations based on specified criteria.\n",
    "\n",
    "    For each state, finds the top 5 unique occupations by annual wage,\n",
    "    regardless of the specific area within the state. When an occupation\n",
    "    appears in multiple areas, only the highest-paying instance is included.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned OEWS dataframe\n",
    "        criteria: Selection criteria ('high_wage_tech', 'high_growth', 'regional_strong')\n",
    "    \"\"\"\n",
    "    # Columns to keep in the final output\n",
    "    target_columns = [\n",
    "        \"OCC_TITLE\",\n",
    "        \"AREA_TITLE\",\n",
    "        \"PRIM_STATE\",\n",
    "        \"A_MEAN\",\n",
    "        \"H_MEAN\",\n",
    "        \"TOT_EMP\",\n",
    "        \"JOBS_1000\",\n",
    "        \"H_PCT25\",\n",
    "        \"H_MEDIAN\",\n",
    "        \"H_PCT75\",\n",
    "        \"A_PCT25\",\n",
    "        \"A_MEDIAN\",\n",
    "        \"A_PCT75\",\n",
    "    ]\n",
    "\n",
    "    # Initialize empty list to hold selected rows\n",
    "    selected_rows = []\n",
    "\n",
    "    if criteria == \"high_wage_tech\":\n",
    "        # Focus on high-paying tech roles in specific states\n",
    "        tech_keywords = \"|\".join([\"Software\", \"Data\", \"Computer\", \"Information\", \"Web\"])\n",
    "        tech_occupations = df[\n",
    "            df[\"OCC_TITLE\"].str.contains(tech_keywords, regex=True, case=False, na=False)\n",
    "        ]\n",
    "\n",
    "        # Get top-paying tech roles in OR and CA\n",
    "        for state in [\"OR\", \"CA\"]:\n",
    "            state_tech = tech_occupations[tech_occupations[\"PRIM_STATE\"] == state]\n",
    "\n",
    "            if not state_tech.empty:\n",
    "                # Group by OCC_TITLE and find the highest paying instance of each occupation\n",
    "                # This ensures we get unique occupations regardless of area\n",
    "                top_occs = (\n",
    "                    state_tech.groupby(\"OCC_TITLE\")\n",
    "                    .apply(lambda x: x.nlargest(1, \"A_MEAN\"))\n",
    "                    .reset_index(drop=True)\n",
    "                )\n",
    "                # Now get the top 5 unique occupations by annual wage\n",
    "                top_roles = top_occs.nlargest(5, \"A_MEAN\")\n",
    "                selected_rows.append(top_roles)\n",
    "\n",
    "                print(f\"Selected {len(top_roles)} unique top-paying occupations from {state}\")\n",
    "            else:\n",
    "                print(f\"Warning: No tech occupations found for state {state}\")\n",
    "\n",
    "    # Combine all selected rows into a single DataFrame\n",
    "    if selected_rows:\n",
    "        target_df = pd.concat(selected_rows)\n",
    "\n",
    "        # Add a rationale column\n",
    "        target_df[\"RATIONALE\"] = target_df.apply(\n",
    "            lambda row: f\"High-paying tech role in {row['PRIM_STATE']}\", axis=1\n",
    "        )\n",
    "\n",
    "        # Use only columns that are actually in our data\n",
    "        return target_df[target_columns + [\"RATIONALE\"]]\n",
    "    else:\n",
    "        # Create empty dataframe with only available columns\n",
    "        return pd.DataFrame(columns=target_columns + [\"RATIONALE\"])\n",
    "\n",
    "\n",
    "target_df = identify_target_occupations(df_clean)\n",
    "\n",
    "\n",
    "# Format numbers with thousand separator for display\n",
    "def format_thousands(val):\n",
    "    if isinstance(val, (int, float)):\n",
    "        return f\"{val:,.0f}\"\n",
    "    return val\n",
    "\n",
    "\n",
    "# Create a dictionary for formatting numeric columns based on available columns\n",
    "format_dict = {}\n",
    "# Basic columns that should always be there\n",
    "format_dict[\"A_MEAN\"] = format_thousands\n",
    "format_dict[\"H_MEAN\"] = lambda x: f\"${x:.2f}\"\n",
    "format_dict[\"TOT_EMP\"] = format_thousands\n",
    "format_dict[\"JOBS_1000\"] = lambda x: f\"{x:.1f}\" if pd.notnull(x) else \"\"\n",
    "\n",
    "# Add formatting for columns that may or may not be present\n",
    "hourly_cols = [\"H_PCT25\", \"H_MEDIAN\", \"H_PCT75\"]\n",
    "for col in hourly_cols:\n",
    "    format_dict[col] = lambda x: f\"${x:.2f}\" if pd.notnull(x) else \"\"\n",
    "\n",
    "annual_cols = [\"A_PCT25\", \"A_MEDIAN\", \"A_PCT75\"]\n",
    "for col in annual_cols:\n",
    "    format_dict[col] = format_thousands\n",
    "\n",
    "display(target_df.style.format(format_dict).set_caption(\"Target Occupations for Web Scraping\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Explain that this data-driven approach to selecting scraping targets is much more effective than random selection. Ask the class: \"How does this analysis inform our scraping strategy? What additional data might we want to collect for these specific roles?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights and Next Steps\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "Let's summarize what we've learned from our exploratory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"=== EXPLORATORY DATA ANALYSIS SUMMARY ===\")\n",
    "print(f\"Total occupations analyzed: {df_clean['OCC_TITLE'].nunique()}\")\n",
    "print(f\"States covered: {df_clean['PRIM_STATE'].nunique()}\")\n",
    "print(f\"Average annual wage across all occupations: ${df_clean['A_MEAN'].mean():,.0f}\")\n",
    "print(f\"Highest paying occupation: {df_clean.loc[df_clean['A_MEAN'].idxmax(), 'OCC_TITLE']}\")\n",
    "print(\n",
    "    f\"Most common occupation by employment: {df_clean.groupby('OCC_TITLE')['TOT_EMP'].sum().idxmax()}\"\n",
    ")\n",
    "\n",
    "# Save our target occupations for the next module\n",
    "# Create the data directory if it doesn't exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Print information about the data we're saving\n",
    "print(f\"\\nTarget dataframe shape: {target_df.shape}\")\n",
    "columns_to_save = target_df.columns\n",
    "\n",
    "print(\"\\nSaving the following columns to CSV:\")\n",
    "pp.pprint(list(columns_to_save), compact=True, width=80)\n",
    "\n",
    "# Ensure we handle the case where the dataframe might be empty\n",
    "if not target_df.empty:\n",
    "    target_df.to_csv(DATA_DIR / \"bls_jobs_metro_area_2024.csv\", index=False)\n",
    "\n",
    "    print(\"Target occupations saved to 'data/bls_jobs_metro_area_2024.csv'\")\n",
    "else:\n",
    "    print(\"Warning: No target occupations identified. CSV file not created.\")\n",
    "\n",
    "print(\"Ready for targeted web scraping in Chapter 2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Transition to Web Scraping\n",
    "\n",
    "> **Instructor Cue:** This is a perfect transition moment. Emphasize that we now have a clear, data-driven rationale for our scraping targets. Ask: \"What specific information might we want to collect about these roles that isn't in the BLS data?\"\n",
    "\n",
    "Our exploratory analysis has revealed:\n",
    "\n",
    "1. **Regional Specialization**: Different states show clear patterns in high-paying occupations\n",
    "2. **Employment vs. Wage Dynamics**: Understanding the relationship between job availability and compensation\n",
    "3. **Target Identification**: Data-driven selection of specific occupation-state combinations\n",
    "\n",
    "In the next chapter, we'll use web scraping to collect real-time job posting data for our identified targets, adding depth to our government dataset with current market information including specific requirements, company details, and salary ranges.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Practice Exercise\n",
    "\n",
    "> **Instructor Cue:** This is a 5-minute exercise to reinforce the analysis concepts. Have students work in pairs and share their findings quickly.\n",
    "\n",
    "**Exercise: Identify Your Own Scraping Target**\n",
    "\n",
    "Using the analysis functions we've built, pick two states of your choice and identify one high-paying occupation that would be interesting to scrape in Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick exercise: Choose your states and find an interesting target\n",
    "my_states = [\"WA\", \"FL\", \"OH\"]  # Replace with your choice\n",
    "\n",
    "# Use our existing functions to analyze these states\n",
    "fig, my_wage_data = create_wage_comparison_chart(df_clean, my_states, top_n=10)\n",
    "plt.show()\n",
    "\n",
    "# Pick one occupation that interests you for scraping\n",
    "print(\"\\nMy chosen target for web scraping:\")\n",
    "print(\"Occupation: [Your choice here]\")\n",
    "print(\"State: [Your choice here]\")\n",
    "print(\"Rationale: [Why this is interesting]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Have 2-3 students quickly share their chosen targets. This will give us variety in the scraping exercise in Chapter 2.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Chapter 1*\n",
    "\n",
    "> **Instructor Cue:** Before moving to Chapter 2, do a quick check-in: \"Any questions about the analysis we just performed? Are you feeling confident about the patterns we identified?\" This is also a good time for a 10-minute break."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,md",
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "nsbe-pdc2025-python-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
