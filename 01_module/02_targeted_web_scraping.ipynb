{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chapter 2: Targeted Web Scraping with Playwright\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Set up and configure Playwright for automated web scraping\n",
    "- Navigate job search websites programmatically\n",
    "- Extract structured data from dynamic web pages\n",
    "- Handle common web scraping challenges (rate limiting, dynamic content)\n",
    "- Save scraped data in a format suitable further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to Web Scraping\n",
    "\n",
    "> **Instructor Cue:** Begin by pulling up Indeed.com in your browser. Perform a manual search for \"Data Scientists\" in Oregon to show the class what we're trying to automate. Point out the various elements we want to extract and discuss the challenges of manual data collection at scale.\n",
    "\n",
    "Based on our exploratory data analysis, we identified high-value target occupations in specific states. While the BLS OEWS data provides excellent foundational insights, it lacks the granular, real-time information that current job postings offer, such as:\n",
    "\n",
    "- Specific skill requirements\n",
    "- Company details and culture information\n",
    "- Exact salary ranges and benefits\n",
    "- Remote work options\n",
    "- Educational requirements\n",
    "\n",
    "**The Problem with websites nowadays ðŸ˜’**\n",
    "\n",
    "The modern web is dynamic. Many websites, like Indeed.com, use JavaScript to load content after the initial page loads. A simple tool like the `requests` library can't see this content because it only gets the initial HTML.\n",
    "\n",
    "Let's first see what happens when we try the traditional approach with `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "response = requests.get(\"https://www.indeed.com/jobs?q=data+analyst&l=Beaverton%2C+OR\")\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Page title: {soup.title.text if soup.title else 'No title found'}\")\n",
    "print(f\"Total HTML length: {len(response.text):,} characters\")\n",
    "\n",
    "# Look for job listings\n",
    "job_cards = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "print(f\"Job cards found with requests: {len(job_cards)}\")\n",
    "\n",
    "display(HTML(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "As you can see, requests can access the basic HTML, but the job listings are loaded dynamically with JavaScript after the page loads, and we are being blocked by Cloudflare's anti-scraping measures.\n",
    "\n",
    "This is where Playwright shines ðŸ’ªðŸ¾!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Why Playwright Over Other Tools?\n",
    "\n",
    "> **Instructor Cue:** Ask the audience: \"Has anyone used BeautifulSoup or Selenium before? What challenges did you encounter?\" Use their responses to highlight Playwright's advantages.\n",
    "\n",
    "Playwright offers several advantages for modern web scraping:\n",
    "\n",
    "1. **Fast and Reliable**: Built for modern web applications\n",
    "2. **Handles JavaScript**: Executes dynamic content automatically\n",
    "3. **Multiple Browser Support**: Chromium, Firefox, and Safari\n",
    "4. **Built-in Waiting**: Intelligent waiting for elements to load\n",
    "5. **Robust Error Handling**: Better handling of network issues and timeouts\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Started Web Scraping with Playwright\n",
    "\n",
    "Let's start by installing and configuring Playwright for our scraping task:\n",
    "\n",
    "**Playwright** is a Python library that automates browser actions. It can launch a browser, navigate to pages, click buttons, and read content after all the JavaScript has finished running. This makes it perfect for scraping modern websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For workshop purposes, these commands are commented out.\n",
    "# !playwright install chromium\n",
    "# For this workshop, we use `uv sync` to install all dependencies from pyproject.toml.\n",
    "# If you haven't set up your environment yet, run this in your terminal:\n",
    "#   uv sync\n",
    "#   playwright install chromium\n",
    "# This will ensure all required packages (including patchright, pandas, etc.) are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import patchright first (which works better with Indeed), fall back to playwright\n",
    "try:\n",
    "    from patchright.async_api import async_playwright\n",
    "\n",
    "    print(\"Using patchright for better compatibility with Indeed\")\n",
    "except ImportError:\n",
    "    print(\"Patchright not found, falling back to standard playwright\")\n",
    "    from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\").absolute()\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Walk through the installation process step by step. If anyone encounters installation issues, help them troubleshoot. Explain that we're using the async version of Playwright for better performance and compatiability with jupyter notebooks\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1. Building Our Job Scraper Class\n",
    "\n",
    "We'll define a class to encapsulate our scraping logic. This is a good practice that keeps our code organized and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Step1_IndeedJobScraper:\n",
    "    \"\"\"\n",
    "    Step 1: Initial skeleton for our Indeed.com job scraper class using Playwright/Patchright (Async API for Jupyter).\n",
    "    This class sets up the basic structure for future scraping logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        job_title: str = \"Data Analyst\",\n",
    "        location: str = \"Beaverton, OR\",\n",
    "        headless: bool = False,\n",
    "    ):\n",
    "        self.job_title = job_title\n",
    "        self.location = location\n",
    "        self.headless = headless\n",
    "\n",
    "    async def run(self) -> list:\n",
    "        \"\"\"Run the Indeed job scraper.\n",
    "\n",
    "        Returns:\n",
    "            list: List of job dictionaries\n",
    "        \"\"\"\n",
    "        # Our scraping logic will go here\n",
    "        job_listings = []\n",
    "\n",
    "        print(f\"ðŸ” Starting scraper for '{self.job_title}' in '{self.location}'...\")\n",
    "\n",
    "        return job_listings\n",
    "\n",
    "\n",
    "# Test our job scraper\n",
    "job_scraper = _Step1_IndeedJobScraper()\n",
    "# test_jobs = await job_scraper.run()\n",
    "# print(f\"ðŸ“‹ Scraper ready! Currently returns {len(test_jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Step 2. Launching The Browser\n",
    "\n",
    "> **Instructor Cue:** Explain the importance of responsible scraping practices. Discuss rate limiting, robots.txt files, and ethical considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import Browser, Page, Playwright\n",
    "\n",
    "\n",
    "class _Step2_IndeedJobScraper(_Step1_IndeedJobScraper):\n",
    "    \"\"\"\n",
    "    Step 2: Enhanced job scraper for Indeed.com using Playwright/Patchright (Async API for Jupyter).\n",
    "    This class builds upon the initial skeleton by adding functionality to launch the browser.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.browser: Browser = None\n",
    "        self.playwright: Playwright = None\n",
    "        self.page: Page = None\n",
    "\n",
    "    async def init_browser(self):\n",
    "        \"\"\"Get a browser instance using Playwright (Async API for Jupyter)\"\"\"\n",
    "\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch_persistent_context(\n",
    "            user_data_dir=\"./.browser_data\",\n",
    "            channel=\"chromium\",\n",
    "            headless=self.headless,\n",
    "            no_viewport=True,\n",
    "        )\n",
    "\n",
    "    async def run(self):\n",
    "        # Our scraping logic will go here\n",
    "        job_listings = []\n",
    "\n",
    "        print(f\"ðŸ” Starting scraper for '{self.job_title}' in '{self.location}'...\")\n",
    "\n",
    "        try:\n",
    "            await self.init_browser()\n",
    "\n",
    "            self.page = await self.browser.new_page()\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "        finally:\n",
    "            # Close manually\n",
    "            await self.browser.close()\n",
    "            await self.playwright.stop()\n",
    "\n",
    "        return job_listings\n",
    "\n",
    "\n",
    "# Test our job scraper\n",
    "job_scraper = _Step2_IndeedJobScraper()\n",
    "# test_jobs = await job_scraper.run()\n",
    "# print(f\"ðŸ“‹ Scraper ready! Currently returns {len(test_jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Step 3. Defining our Job Search Logic\n",
    "\n",
    "> **Instructor Cue:** Open the browser developer tools and show the class how to inspect elements to find the CSS selectors we'll use. This is a great hands-on moment to demonstrate how web scraping detective work happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Step3_IndeedJobScraper(_Step2_IndeedJobScraper):\n",
    "    \"\"\"\n",
    "    Step 3: Job scraper for Indeed.com using Playwright/Patchright (Async API for Jupyter).\n",
    "    This class builds upon the previous steps by adding functionality to fill the job search form.\n",
    "    \"\"\"\n",
    "\n",
    "    async def fill_job_search_form(self):\n",
    "        \"\"\"\n",
    "        Fill the job search form on Indeed.\n",
    "        \"\"\"\n",
    "        # Navigate and search\n",
    "        print(\"ðŸŒ Navigating to Indeed...\")\n",
    "        await self.page.goto(\"https://www.indeed.com\", timeout=60000)\n",
    "\n",
    "        print(\"ðŸ“ Filling search form...\")\n",
    "        # Fill in Job Title\n",
    "        await self.page.locator('input[name=\"q\"]').click()\n",
    "        await self.page.locator('input[name=\"q\"]').fill(self.job_title)\n",
    "\n",
    "        # Clear the default location before filling\n",
    "        # await self.page.locator('input[name=\"l\"]').press(\"Control+A\")\n",
    "        # await self.page.locator('input[name=\"l\"]').press(\"Delete\")\n",
    "\n",
    "        # Fill in Location\n",
    "        await self.page.locator('input[name=\"l\"]').click()\n",
    "        await self.page.locator('input[name=\"l\"]').fill(self.location)\n",
    "\n",
    "        await asyncio.sleep(2)  # Small wait to ensure input is registered\n",
    "\n",
    "        await self.page.click('button[type=\"submit\"]')\n",
    "\n",
    "        print(\"â³ Waiting for page to load...\")\n",
    "        await asyncio.sleep(3)\n",
    "        await self.page.wait_for_selector(\".jobsearch-LeftPane #mosaic-jobResults\")\n",
    "\n",
    "    async def run(self):\n",
    "        # Our scraping logic will go here\n",
    "        job_listings = []\n",
    "\n",
    "        print(f\"ðŸ” Starting scraper for '{self.job_title}' in '{self.location}'...\")\n",
    "\n",
    "        try:\n",
    "            await self.init_browser()\n",
    "\n",
    "            self.page = await self.browser.new_page()\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            await self.fill_job_search_form()\n",
    "\n",
    "        finally:\n",
    "            # Close manually\n",
    "            await self.browser.close()\n",
    "            await self.playwright.stop()\n",
    "\n",
    "        return job_listings\n",
    "\n",
    "\n",
    "# Test our job scraper\n",
    "job_scraper = _Step3_IndeedJobScraper(\n",
    "    job_title=\"Cyber Security Engineer\", location=\"New Orleans, LA\"\n",
    ")\n",
    "# test_jobs = await job_scraper.run()\n",
    "# print(f\"ðŸ“‹ Scraper ready! Currently returns {len(test_jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Step 4. Define Job Detail Extraction Logic\n",
    "\n",
    "> **Instructor Cue:** Point out the defensive programming practices here - checking if elements exist before accessing them, handling exceptions gracefully. Ask the class: \"Why is this error handling so important in web scraping?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Step4_IndeedJobScraper(_Step3_IndeedJobScraper):\n",
    "    \"\"\"\n",
    "    Step 4: Job scraper for Indeed.com using Playwright/Patchright (Async API for Jupyter).\n",
    "    This class builds upon the previous steps by adding functionality to extract job listings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_results: int = 10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.max_results = max_results\n",
    "        self._timeout_ms = 1500  # small timeout so we don't hang forever\n",
    "\n",
    "    async def extract_job_listings(self, job_listings: list):\n",
    "        \"\"\"\n",
    "        Extract job listings from the search results page.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"ðŸ” Looking for job listings...\")\n",
    "\n",
    "        job_cards = self.page.locator(\".cardOutline\")\n",
    "        card_count = await job_cards.count()\n",
    "\n",
    "        if card_count == 0:\n",
    "            print(\"âŒ No job cards found. Website structure may have changed.\")\n",
    "            return job_listings\n",
    "\n",
    "        print(f\"âœ… Found {card_count} jobs. Extracting data from the first {self.max_results}...\\n\")\n",
    "\n",
    "        for i in range(min(card_count, self.max_results)):\n",
    "            card = job_cards.nth(i)\n",
    "            job_data = {\n",
    "                \"job_title\": None,\n",
    "                \"company_name\": None,\n",
    "                \"location\": None,\n",
    "                \"salary\": None,\n",
    "                \"job_description\": None,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Click the card to load the description (only if it exists)\n",
    "                if await card.count() == 0:\n",
    "                    print(f\"  âš ï¸ Card {i + 1} not found; skipping...\")\n",
    "                    continue\n",
    "\n",
    "                await card.click(timeout=self._timeout_ms)\n",
    "\n",
    "                # Wait for the job description panel to load its content\n",
    "                await asyncio.sleep(1.5)\n",
    "\n",
    "                # == Job Title ==\n",
    "                title_elem = card.locator(\"h2 a span\")\n",
    "                if await title_elem.count() > 0:\n",
    "                    title_attr = await title_elem.get_attribute(\"title\", timeout=self._timeout_ms)\n",
    "\n",
    "                    if title_attr:\n",
    "                        job_data[\"job_title\"] = title_attr\n",
    "                    else:\n",
    "                        try:\n",
    "                            job_data[\"job_title\"] = await title_elem.inner_text(\n",
    "                                timeout=self._timeout_ms\n",
    "                            )\n",
    "                        except Exception:\n",
    "                            job_data[\"job_title\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i + 1}] Job Title element missing.\")\n",
    "\n",
    "                # == Company Name ==\n",
    "                company_elem = card.locator('[data-testid=\"company-name\"]')\n",
    "                if await company_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"company_name\"] = await company_elem.inner_text(\n",
    "                            timeout=self._timeout_ms\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        job_data[\"company_name\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i + 1}] Company name missing.\")\n",
    "\n",
    "                # == Location ==\n",
    "                location_elem = card.locator('[data-testid=\"text-location\"]')\n",
    "                if await location_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"location\"] = await location_elem.inner_text(\n",
    "                            timeout=self._timeout_ms\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        job_data[\"location\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i + 1}] Location missing.\")\n",
    "\n",
    "                # == Salary ==\n",
    "                company_div = card.locator('div[class*=\"company_location\"]')\n",
    "                if await company_div.count() > 0:\n",
    "                    salary_range_div = company_div.locator(\"+ div\")\n",
    "                    if await salary_range_div.count() > 0:\n",
    "                        try:\n",
    "                            job_data[\"salary\"] = (\n",
    "                                await salary_range_div.inner_text(timeout=self._timeout_ms)\n",
    "                            ).strip()\n",
    "                        except Exception:\n",
    "                            job_data[\"salary\"] = None\n",
    "                    else:\n",
    "                        print(f\"\\tâ„¹ï¸ [{i + 1}] Salary information missing.\")\n",
    "\n",
    "                # == Job Description ==\n",
    "                description_elem = self.page.locator(\"#jobDescriptionText\")\n",
    "                if await description_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"job_description\"] = (\n",
    "                            await description_elem.inner_text(timeout=self._timeout_ms)\n",
    "                        ).strip()\n",
    "                    except Exception:\n",
    "                        job_data[\"job_description\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i + 1}] Job Description missing.\")\n",
    "\n",
    "                job_listings.append(job_data)\n",
    "\n",
    "                if all(job_data.values()):\n",
    "                    print(f\"\\t=> ðŸ“ [{i + 1}] Extracted: {job_data['job_title']}\")\n",
    "                else:\n",
    "                    print(f\"\\t=> ðŸ“ Extracted Job [{i + 1}] Details with Missing Fields\")\n",
    "\n",
    "                print(\"-\" * 60)\n",
    "            except Exception as e:\n",
    "                print(f\"\\tâš ï¸ Error extracting job {i + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    async def run(self):\n",
    "        # Our scraping logic will go here\n",
    "        job_listings = []\n",
    "\n",
    "        print(f\"ðŸ” Starting scraper for '{self.job_title}' in '{self.location}'...\")\n",
    "\n",
    "        try:\n",
    "            await self.init_browser()\n",
    "\n",
    "            self.page = await self.browser.new_page()\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            await self.fill_job_search_form()\n",
    "            await self.extract_job_listings(job_listings)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during scraping: {e}\")\n",
    "            return job_listings\n",
    "        finally:\n",
    "            print(f\"ðŸ“Š Extracted a total of {len(job_listings)} job listings\")\n",
    "\n",
    "            # Close manually\n",
    "            await self.browser.close()\n",
    "            await self.playwright.stop()\n",
    "\n",
    "            print(\"ðŸ”’ Browser closed\")\n",
    "\n",
    "        return job_listings\n",
    "\n",
    "\n",
    "# Test our job scraper\n",
    "job_scraper = _Step4_IndeedJobScraper(\n",
    "    job_title=\"Data Scientist\", location=\"San Jose, CA\", max_results=15\n",
    ")\n",
    "test_jobs = await job_scraper.run()\n",
    "\n",
    "print(f\"\\nðŸ“‹ Scraper ready! Currently returns {len(test_jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Show the test_jobs data structure to the class. Point out that we have a list of dictionaries, each representing a job. Mention that this is perfect for converting to a pandas DataFrame and saving as CSV. Ask: \"What advantages does CSV format give us for data analysis?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint as pp; pp.pprint(test_jobs, compact=True, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Save Scraper to Script\n",
    "\n",
    "> **Instructor Cue:** Ask the class: \"Why save our code to a separate file instead of keeping it in the notebook?\" Take responses, then explain the benefits below.\n",
    "\n",
    "We've built a working scraper through experimentation. Now let's make it reusable by saving it to our `workshoplib` package.\n",
    "\n",
    "**Benefits of Modular Code:**\n",
    "- **Reuse across modules** - Import our scraper in Module 2 instead of copy-pasting code\n",
    "- **Share with others** - Your `workshoplib` becomes a professional toolkit\n",
    "- **Maintain and improve** - Update the library without breaking existing notebooks\n",
    "- **Industry practice** - Professional teams organize code this way\n",
    "\n",
    "**Why Build It Incrementally?**\n",
    "\n",
    "We use `%%writefile` first, then `%%writefile -a` (append) to:\n",
    "- Show clear structure: imports â†’ class â†’ functions\n",
    "- Understand dependencies between components\n",
    "- Practice proper Python module organization\n",
    "- Make debugging easier when issues arise\n",
    "\n",
    "> **Instructor Cue:** Emphasize that this mirrors real development work - code is organized into logical, reusable components.\n",
    "\n",
    "Let's save our scraper for use throughout the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../workshoplib/src/workshoplib/indeed_scraper.py\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from playwright.async_api import Browser, Page, Playwright\n",
    "\n",
    "# Try to import patchright first (which works better with Indeed), fall back to playwright\n",
    "try:\n",
    "    from patchright.async_api import async_playwright\n",
    "    print(\"Using patchright for better compatibility with Indeed\")\n",
    "except ImportError:\n",
    "    print(\"Patchright not found, falling back to standard playwright\")\n",
    "    from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Then let's save our `IndeedJobScraper` class that we built interatively overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../workshoplib/src/workshoplib/indeed_scraper.py\n",
    "\n",
    "class IndeedJobScraper:\n",
    "    \"\"\"\n",
    "    Job scraper for Indeed.com using Playwright/Patchright (Async API for Jupyter).\n",
    "    \"\"\"\n",
    "    def __init__(self, job_title: str, location: str, headless: bool = True, max_results: int = 10):\n",
    "        self.job_title = job_title\n",
    "        self.location = location\n",
    "        self.headless = headless\n",
    "        self.browser: Browser = None\n",
    "        self.playwright: Playwright = None\n",
    "        self.page: Page = None\n",
    "        self.max_results = max_results\n",
    "        self._timeout_ms = 1500  # small timeout so we don't hang forever\n",
    "\n",
    "    async def init_browser(self):\n",
    "        \"\"\"Get a browser instance using Playwright (Async API for Jupyter)\n",
    "        \"\"\"\n",
    "\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch_persistent_context(\n",
    "            user_data_dir=\"./.browser_data\",\n",
    "            channel=\"chromium\",\n",
    "            headless=self.headless,\n",
    "            no_viewport=True,\n",
    "        )\n",
    "\n",
    "    async def fill_job_search_form(self):\n",
    "        \"\"\"\n",
    "        Fill the job search form on Indeed.\n",
    "        \"\"\"\n",
    "        # Navigate and search\n",
    "        print(\"ðŸŒ Navigating to Indeed...\")\n",
    "        await self.page.goto(\"https://www.indeed.com\", timeout=60000)\n",
    "\n",
    "        print(\"ðŸ“ Filling search form...\")\n",
    "        # Fill in Job Title\n",
    "        await self.page.locator('input[name=\"q\"]').click()\n",
    "        await self.page.locator('input[name=\"q\"]').fill(self.job_title)\n",
    "\n",
    "        # Clear the default location before filling\n",
    "        # await self.page.locator('input[name=\"l\"]').press(\"Control+A\")\n",
    "        # await self.page.locator('input[name=\"l\"]').press(\"Delete\")\n",
    "\n",
    "        # Fill in Location\n",
    "        await self.page.locator('input[name=\"l\"]').click()\n",
    "        await self.page.locator('input[name=\"l\"]').fill(self.location)\n",
    "\n",
    "        await asyncio.sleep(2)  # Small wait to ensure input is registered\n",
    "\n",
    "        await self.page.click('button[type=\"submit\"]')\n",
    "\n",
    "        print(\"â³ Waiting for page to load...\")\n",
    "        await asyncio.sleep(3)\n",
    "        await self.page.wait_for_selector(\".jobsearch-LeftPane #mosaic-jobResults\")\n",
    "\n",
    "    async def extract_job_listings(self, job_listings: list):\n",
    "        \"\"\"\n",
    "        Extract job listings from the search results page.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"ðŸ” Looking for job listings...\")\n",
    "\n",
    "        job_cards = self.page.locator('.cardOutline')\n",
    "        card_count = await job_cards.count()\n",
    "\n",
    "        if card_count == 0:\n",
    "            print(\"âŒ No job cards found. Website structure may have changed.\")\n",
    "            return job_listings\n",
    "\n",
    "        print(f\"âœ… Found {card_count} jobs. Extracting data from the first {self.max_results}...\\n\")\n",
    "\n",
    "        for i in range(min(card_count, self.max_results)):\n",
    "            card = job_cards.nth(i)\n",
    "            job_data = {\n",
    "                \"job_title\": None,\n",
    "                \"company_name\": None,\n",
    "                \"location\": None,\n",
    "                \"salary\": None,\n",
    "                \"job_description\": None\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Click the card to load the description (only if it exists)\n",
    "                if await card.count() == 0:\n",
    "                    print(f\"  âš ï¸ Card {i+1} not found; skipping...\")\n",
    "                    continue\n",
    "\n",
    "                await card.click(timeout=self._timeout_ms)\n",
    "\n",
    "                # Wait for the job description panel to load its content\n",
    "                await asyncio.sleep(1.5)\n",
    "\n",
    "                # == Job Title ==\n",
    "                title_elem = card.locator('h2 a span')\n",
    "                if await title_elem.count() > 0:\n",
    "                    title_attr = await title_elem.get_attribute('title', timeout=self._timeout_ms)\n",
    "\n",
    "                    if title_attr:\n",
    "                        job_data[\"job_title\"] = title_attr\n",
    "                    else:\n",
    "                        try:\n",
    "                            job_data[\"job_title\"] = await title_elem.inner_text(timeout=self._timeout_ms)\n",
    "                        except Exception:\n",
    "                            job_data[\"job_title\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i+1}] Job Title element missing.\")\n",
    "\n",
    "                # == Company Name ==\n",
    "                company_elem = card.locator('[data-testid=\"company-name\"]')\n",
    "                if await company_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"company_name\"] = await company_elem.inner_text(timeout=self._timeout_ms)\n",
    "                    except Exception:\n",
    "                        job_data[\"company_name\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i+1}] Company name missing.\")\n",
    "\n",
    "                # == Location ==\n",
    "                location_elem = card.locator('[data-testid=\"text-location\"]')\n",
    "                if await location_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"location\"] = await location_elem.inner_text(timeout=self._timeout_ms)\n",
    "                    except Exception:\n",
    "                        job_data[\"location\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i+1}] Location missing.\")\n",
    "\n",
    "                # == Salary ==\n",
    "                company_div = card.locator('div[class*=\"company_location\"]')\n",
    "                if await company_div.count() > 0:\n",
    "                    salary_range_div = company_div.locator('+ div')\n",
    "                    if await salary_range_div.count() > 0:\n",
    "                        try:\n",
    "                            job_data[\"salary\"] = (await salary_range_div.inner_text(timeout=self._timeout_ms)).strip()\n",
    "                        except Exception:\n",
    "                            job_data[\"salary\"] = None\n",
    "                    else:\n",
    "                        print(f\"\\tâ„¹ï¸ [{i+1}] Salary information missing.\")\n",
    "\n",
    "                # == Job Description ==\n",
    "                description_elem = self.page.locator('#jobDescriptionText')\n",
    "                if await description_elem.count() > 0:\n",
    "                    try:\n",
    "                        job_data[\"job_description\"] = (await description_elem.inner_text(timeout=self._timeout_ms)).strip()\n",
    "                    except Exception:\n",
    "                        job_data[\"job_description\"] = None\n",
    "                else:\n",
    "                    print(f\"\\tâ„¹ï¸ [{i+1}] Job Description missing.\")\n",
    "\n",
    "                job_listings.append(job_data)\n",
    "\n",
    "                if all(job_data.values()):\n",
    "                    print(f\"\\t=> ðŸ“ [{i+1}] Extracted: {job_data['job_title']}\")\n",
    "                else:\n",
    "                    print(f\"\\t=> ðŸ“ Extracted Job [{i+1}] Details with Missing Fields\")\n",
    "\n",
    "                print(\"-\" * 60)\n",
    "            except Exception as e:\n",
    "                print(f\"\\tâš ï¸ Error extracting job {i+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    async def run(self):\n",
    "        # Our scraping logic will go here\n",
    "        job_listings = []\n",
    "\n",
    "        print(f\"ðŸ” Starting scraper for '{self.job_title}' in '{self.location}'...\")\n",
    "\n",
    "        try:\n",
    "            await self.init_browser()\n",
    "\n",
    "            self.page = await self.browser.new_page()\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            await self.fill_job_search_form()\n",
    "            await self.extract_job_listings(job_listings)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during scraping: {e}\")\n",
    "            return job_listings\n",
    "        finally:\n",
    "            print(f\"ðŸ“Š Extracted a total of {len(job_listings)} job listings\")\n",
    "\n",
    "            # Close manually\n",
    "            await self.browser.close()\n",
    "            await self.playwright.stop()\n",
    "\n",
    "            print(\"ðŸ”’ Browser closed\")\n",
    "\n",
    "        return job_listings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5. Saving Job Data to CSV File\n",
    "\n",
    "Now that we have our job data extracted, we need to save it in a format that's easy to work with in our next modules. CSV files are perfect for this because they're:\n",
    "- Easy to read with pandas\n",
    "- Compatible with Excel and other tools\n",
    "- Human-readable for quick inspection\n",
    "- Small file size for efficient storage\n",
    "\n",
    "Let's create a simple function to save our job data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../workshoplib/src/workshoplib/indeed_scraper.py\n",
    "\n",
    "def save_jobs2csv(job_data: list, job_title: str, location: str, data_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Save a list of job dictionaries to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        job_data: List of dictionaries containing job information\n",
    "        job_title: The job title used in the search (for filename)\n",
    "        location: The location used in the search (for filename)\n",
    "        data_dir: Directory to save the file (defaults to DATA_DIR)\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    # Step 1: Check if we have data to save\n",
    "    if not job_data:\n",
    "        print(\"âš ï¸ No job data provided. Nothing to save.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"ðŸ“‹ Converting {len(job_data)} job records to DataFrame...\")\n",
    "\n",
    "    # Step 2: Create DataFrame from our list of job dictionaries\n",
    "    df = pd.DataFrame(job_data)\n",
    "\n",
    "    # Step 3: Create a clean filename from job_title and location\n",
    "    # Remove special characters that could cause file system problems\n",
    "    clean_job_title = re.sub(r'[^\\w\\s-]', '', job_title.strip()).replace(' ', '_').lower()\n",
    "    clean_location = re.sub(r'[^\\w\\s-]', '', location.strip()).replace(' ', '_').lower()\n",
    "\n",
    "    # Step 4: Create the filename using our specified format\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"scraped_indeed_{clean_job_title}_{clean_location}_jobs_{timestamp}.csv\"\n",
    "\n",
    "    # Step 5: Create the full file path\n",
    "    file_path = data_dir / filename\n",
    "\n",
    "    # Step 6: Save to CSV (index=False means don't save row numbers)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"ðŸ’¾ Saved {len(job_data)} jobs to: {file_path}\")\n",
    "    print(f\"ðŸ“Š Data shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"ðŸ“‚ File size: {file_path.stat().st_size:,} bytes\")\n",
    "\n",
    "    return str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshoplib.indeed_scraper import save_jobs2csv\n",
    "\n",
    "if test_jobs:\n",
    "    print(\"ðŸš€ Testing our save function...\")\n",
    "    saved_file = save_jobs2csv(\n",
    "        job_data=test_jobs,\n",
    "        job_title=job_scraper.job_title,\n",
    "        location=job_scraper.location,\n",
    "        data_dir=DATA_DIR,\n",
    "    )\n",
    "\n",
    "    # Let's quickly inspect what we saved\n",
    "    print(\"\\nðŸ“‹ Quick preview of saved data:\")\n",
    "    df_preview = pd.read_csv(saved_file)\n",
    "    display(df_preview.head())  # Show first couple of rows\n",
    "    print(f\"\\nðŸ·ï¸ Column names: {list(df_preview.columns)}\")\n",
    "    print(f\"ðŸ“Š Data types:\\n{df_preview.dtypes}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No job data to save. Run the scraper first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Walk through each step of the function slowly. Point to the filename creation process - show how \"Data Analyst\" becomes \"data_analyst\" and \"New Orleans, LA\" becomes \"new_orleans_la\". Ask the class: \"Why do we need to clean these strings for filenames?\" Emphasize that `index=False` prevents pandas from adding row numbers to our CSV.\n",
    "\n",
    "#### Quick Sanity Check\n",
    "\n",
    "Let's also verify our saved file and see what we accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Let's see what files we've created in our data directory:\")\n",
    "for p in sorted(\n",
    "    DATA_DIR.glob(\"scraped_indeed_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True\n",
    "):\n",
    "    file_size = p.stat().st_size\n",
    "    modified = datetime.fromtimestamp(p.stat().st_mtime)\n",
    "    print(f\"   ðŸ“„ {p.name}\")\n",
    "    print(f\"      Size: {file_size:,} bytes\")\n",
    "    print(f\"      Modified: {modified}\")\n",
    "    print()\n",
    "\n",
    "# Quick data inspection\n",
    "if \"saved_file\" in locals() and saved_file:\n",
    "    print(\"ðŸ” Quick data quality check:\")\n",
    "    df_check = pd.read_csv(saved_file)\n",
    "    print(f\"   ðŸ“Š Total jobs saved: {len(df_check)}\")\n",
    "\n",
    "    # Check for missing data (we'll clean this in Module 2!)\n",
    "    for col in df_check.columns:\n",
    "        missing_count = df_check[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df_check)) * 100\n",
    "        print(f\"   ðŸ“ˆ {col}: {missing_count} missing values ({missing_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Show the file in the data directory. Open it in a text editor or Excel to demonstrate that it's a standard CSV. Point out the missing data percentages and say: \"Don't worry about these missing values - in Module 2, we'll use AI to help us clean and organize this messy real-world data!\"\n",
    "\n",
    "## Chapter 2 Wrap-Up: From Web to Structured Data\n",
    "\n",
    "> **Instructor Cue:** Take a moment to celebrate with the class! Show them the CSV file they just created. Open it in Excel or a text editor and point out both the successful data extraction AND the messy, inconsistent nature of real-world data.\n",
    "\n",
    "### ðŸŽ‰ **What We've Accomplished Together**\n",
    "\n",
    "In just one chapter, we've built something powerful:\n",
    "\n",
    "**ðŸ”§ Technical Wins:**\n",
    "- âœ… **Conquered Modern Web Scraping** - Moved beyond simple requests to handle JavaScript-heavy sites\n",
    "- âœ… **Built a Robust Scraper Class** - 5 iterative steps from concept to working code\n",
    "- âœ… **Extracted Real Job Data** - Not toy examples, but actual Indeed.com listings with all their complexity\n",
    "- âœ… **Created Reusable Tools** - Our `IndeedJobScraper` is now part of `workshoplib` for future use\n",
    "\n",
    "**ðŸ“Š Data Discovery Success:**\n",
    "- âœ… **Captured 5 Critical Fields** - Job titles, companies, locations, salaries, and full descriptions\n",
    "- âœ… **Preserved Raw Reality** - We kept the data exactly as we found it, warts and all\n",
    "- âœ… **Built a Data Pipeline** - From web page to structured CSV in minutes\n",
    "\n",
    "### ðŸ¤” **But Wait... There's a Problem!**\n",
    "\n",
    "> **Instructor Cue:** Ask the class to look at their CSV data. Point out issues like: \"Senior Data Scientist\" vs \"Data Scientist - Senior Level\" vs \"Sr. Data Scientist\". Show salary inconsistencies like \"$80K\" vs \"$80,000/year\" vs \"Competitive salary\". This creates the perfect setup for Module 2.\n",
    "\n",
    "Take a closer look at our beautifully extracted data. Notice anything... *messy*?\n",
    "\n",
    "**The Reality of Web-Scraped Data:**\n",
    "- ðŸ” **Job Titles Are Inconsistent** - \"Data Analyst\" vs \"Data Analytics Specialist\" vs \"Jr. Data Analyst\"\n",
    "- ðŸ’° **Salary Formats Vary Wildly** - \"$75,000/year\" vs \"$75K\" vs \"Competitive\" vs missing entirely\n",
    "- ðŸ“ **Location Data Is All Over the Map** - \"Portland, OR\" vs \"Portland, Oregon\" vs \"Portland Metro Area\"\n",
    "- ðŸ“ **Descriptions Are Information Gold Mines** - But buried in paragraphs of unstructured text\n",
    "\n",
    "**This is where most data projects stall out.** ðŸ˜¤\n",
    "\n",
    "Traditionally, you'd spend days or weeks writing complex regex patterns, building lookup tables, and creating custom parsing logic for each field. It's tedious, error-prone, and breaks every time the data format changes slightly.\n",
    "\n",
    "### ðŸš€ **Enter the AI Revolution**\n",
    "\n",
    "> **Instructor Cue:** Build excitement here! This is the transition moment. You're about to show them how AI transforms the most tedious part of data work into something almost magical.\n",
    "\n",
    "**But what if we could have a smart assistant do all that work for us?**\n",
    "\n",
    "What if we could simply tell an AI:\n",
    "- *\"Hey, group these job titles into standard categories\"*\n",
    "- *\"Extract the actual salary numbers from these messy strings\"*\n",
    "- *\"Tell me what skills are mentioned in these job descriptions\"*\n",
    "\n",
    "**That's exactly what we're doing in Module 2!** ðŸŽ¯\n",
    "\n",
    "### ðŸŽª **Coming Up: AI-Powered Data Wrangling**\n",
    "\n",
    "In our next module, we'll transform today's raw, messy data into analysis-ready insights using Google's Gemini AI:\n",
    "\n",
    "**ðŸ§  Module 2 Preview:**\n",
    "- ðŸ“š **Load Our Scraped Data** - Import the CSV we just created and explore its quirks\n",
    "- ðŸ¤– **Meet PydanticAI** - Set up our AI-powered data cleaning assistant  \n",
    "- ðŸ·ï¸ **Smart Job Classification** - Let AI categorize \"Senior Data Scientist II\" and \"Data Science Manager\" into clean, consistent groups\n",
    "- ðŸ’° **Intelligent Salary Parsing** - Transform \"$80K-$100K DOE\" into structured, comparable numbers\n",
    "- ðŸ“Š **Generate Clean Visualizations** - Create publication-ready charts from our newly organized data\n",
    "- ðŸ”— **Merge with BLS Data** - Combine our real-time job market insights with government employment statistics\n",
    "\n",
    "**The Best Part?** You'll write maybe 10 lines of actual data cleaning code. The AI does the heavy lifting while you focus on the insights!\n",
    "\n",
    "> **Instructor Cue:** End with energy and anticipation. Maybe say something like: \"Ready to see what happens when we give AI superpowers to our data? Let's dive into Module 2 and turn this messy pile of job data into crystal-clear insights!\"\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Key Takeaway:** We've proven we can extract data from the modern web. Now let's prove that AI can make sense of it faster and better than any traditional approach. The combination of web scraping + AI is where the real magic happens!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,md",
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "nsbe-pdc2025-python-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
