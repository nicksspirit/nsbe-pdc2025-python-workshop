{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chapter 2: Targeted Web Scraping with Playwright\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Set up and configure Playwright for automated web scraping\n",
    "- Navigate job search websites programmatically\n",
    "- Extract structured data from dynamic web pages\n",
    "- Handle common web scraping challenges (rate limiting, dynamic content)\n",
    "- Save scraped data in a format suitable for machine learning analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Web Scraping Strategy\n",
    "\n",
    "> **Instructor Cue:** Begin by pulling up Indeed.com in your browser. Perform a manual search for \"Data Scientists\" in Oregon to show the class what we're trying to automate. Point out the various elements we want to extract and discuss the challenges of manual data collection at scale.\n",
    "\n",
    "Based on our exploratory data analysis, we identified high-value target occupations in specific states. While the BLS OEWS data provides excellent foundational insights, it lacks the granular, real-time information that current job postings offer, such as:\n",
    "\n",
    "- Specific skill requirements\n",
    "- Company details and culture information  \n",
    "- Exact salary ranges and benefits\n",
    "- Remote work options\n",
    "- Educational requirements\n",
    "\n",
    "### Why Playwright Over Other Tools?\n",
    "\n",
    "> **Instructor Cue:** Ask the audience: \"Has anyone used BeautifulSoup or Selenium before? What challenges did you encounter?\" Use their responses to highlight Playwright's advantages.\n",
    "\n",
    "Playwright offers several advantages for modern web scraping:\n",
    "\n",
    "1. **Fast and Reliable**: Built for modern web applications\n",
    "2. **Handles JavaScript**: Executes dynamic content automatically\n",
    "3. **Multiple Browser Support**: Chromium, Firefox, and Safari\n",
    "4. **Built-in Waiting**: Intelligent waiting for elements to load\n",
    "5. **Robust Error Handling**: Better handling of network issues and timeouts\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Playwright Environment\n",
    "\n",
    "Let's start by installing and configuring Playwright for our scraping task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation commands for Playwright\n",
    "# If you haven't installed these packages yet, uncomment and run:\n",
    "# !pip install playwright pandas requests requests_cache\n",
    "# !pip install git+https://github.com/nsbe-pdc/patchright.git  # Special version for Indeed\n",
    "# !playwright install chromium\n",
    "\n",
    "# NOTE: For workshop purposes, these commands are commented out.\n",
    "# If this is your first time running this notebook, you'll need to install\n",
    "# the required packages. You can either:\n",
    "#   1. Uncomment the lines above and run this cell, OR\n",
    "#   2. Run these commands in a terminal:\n",
    "#      pip install playwright pandas requests requests_cache\n",
    "#      pip install git+https://github.com/nsbe-pdc/patchright.git\n",
    "#      playwright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import patchright first (which works better with Indeed), fall back to playwright\n",
    "try:\n",
    "    from patchright.async_api import async_playwright\n",
    "\n",
    "    print(\"Using patchright for better compatibility with Indeed\")\n",
    "except ImportError:\n",
    "    print(\"Patchright not found, falling back to standard playwright\")\n",
    "    try:\n",
    "        from playwright.async_api import async_playwright\n",
    "\n",
    "        print(\"Using standard playwright\")\n",
    "    except ImportError:\n",
    "        print(\"Playwright not installed. Please install it with:\")\n",
    "        print(\"pip install playwright\")\n",
    "        print(\"playwright install chromium\")\n",
    "\n",
    "# Make sure data directory exists\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Walk through the installation process step by step. If anyone encounters installation issues, help them troubleshoot. Explain that we're using the async version of Playwright for better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Building Our Job Scraper Class\n",
    "\n",
    "Let's create a comprehensive job scraper that can extract all the information we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndeedJobScraper:\n",
    "    \"\"\"\n",
    "    A robust job scraper for Indeed.com using Playwright/Patchright.\n",
    "    Designed to extract comprehensive job posting information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, headless=False, delay_range=(1, 3)):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with configuration options.\n",
    "\n",
    "        Args:\n",
    "            headless: Whether to run browser in headless mode (default: False for better stability with Indeed)\n",
    "            delay_range: Tuple of (min, max) seconds for random delays\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.delay_range = delay_range\n",
    "        self.scraped_jobs = []  # Store scraped jobs\n",
    "        self.browser = None\n",
    "        self.page = None\n",
    "        self.context = None\n",
    "        self.playwright = None\n",
    "        self.user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "\n",
    "    async def start_browser(self):\n",
    "        \"\"\"Initialize browser and page objects with optimized settings for Indeed.\"\"\"\n",
    "        self.playwright = await async_playwright().start()\n",
    "\n",
    "        # Use a persistent context for better stability with Indeed\n",
    "        user_data_dir = Path(\"./browser_data\")\n",
    "        user_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # Try to use the persistent context approach which works better for some sites\n",
    "            self.browser = await self.playwright.chromium.launch_persistent_context(\n",
    "                user_data_dir=str(user_data_dir),\n",
    "                headless=self.headless,  # Using non-headless mode for better results with Indeed\n",
    "                viewport={\"width\": 1280, \"height\": 800},\n",
    "                user_agent=self.user_agent,\n",
    "            )\n",
    "            self.page = await self.browser.new_page()\n",
    "\n",
    "            # Configure page for better scraping\n",
    "            await self.page.set_viewport_size({\"width\": 1280, \"height\": 800})\n",
    "            await self.page.set_extra_http_headers({\"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "\n",
    "            print(\n",
    "                \"Browser started successfully in \"\n",
    "                + (\"headless\" if self.headless else \"visible\")\n",
    "                + \" mode\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to launch persistent context: {e}. Falling back to standard browser.\")\n",
    "            # Fall back to standard browser launch\n",
    "            self.browser = await self.playwright.chromium.launch(headless=self.headless)\n",
    "            self.context = await self.browser.new_context(\n",
    "                viewport={\"width\": 1280, \"height\": 800}, user_agent=self.user_agent\n",
    "            )\n",
    "            self.page = await self.context.new_page()\n",
    "\n",
    "    async def close_browser(self):\n",
    "        \"\"\"Clean up browser resources.\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if hasattr(self, \"playwright\") and self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "    def random_delay(self):\n",
    "        \"\"\"Add random delay to mimic human behavior.\"\"\"\n",
    "        delay = random.uniform(*self.delay_range)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    async def search_jobs(self, job_title, location, max_pages=3):\n",
    "        \"\"\"\n",
    "        Search for jobs on Indeed and return structured data.\n",
    "\n",
    "        Args:\n",
    "            job_title: The job title to search for\n",
    "            location: Location (city, state or state abbreviation)\n",
    "            max_pages: Maximum number of pages to scrape\n",
    "\n",
    "        Returns:\n",
    "            List of job dictionaries\n",
    "        \"\"\"\n",
    "        # Reset the scraped jobs list\n",
    "        self.scraped_jobs = []\n",
    "\n",
    "        print(f\"Starting search for '{job_title}' in '{location}'...\")\n",
    "\n",
    "        try:\n",
    "            # Go directly to Indeed.com\n",
    "            await self.page.goto(\n",
    "                \"https://www.indeed.com\", wait_until=\"domcontentloaded\", timeout=60000\n",
    "            )\n",
    "            await asyncio.sleep(2)  # Small wait for page to stabilize\n",
    "\n",
    "            # Find search form elements\n",
    "            print(\"ðŸ“ Filling search form...\")\n",
    "            await self.page.locator('input[name=\"q\"]').click()\n",
    "            await self.page.locator('input[name=\"q\"]').fill(job_title)\n",
    "\n",
    "            # Clear the location field before filling (if it has a default value)\n",
    "            try:\n",
    "                location_input = self.page.locator('input[name=\"l\"]')\n",
    "                await location_input.click()\n",
    "                await location_input.fill(\"\")  # Clear field\n",
    "                await location_input.fill(location)  # Set new location\n",
    "            except Exception as e:\n",
    "                print(f\"Issue with location field: {e}\")\n",
    "                # Alternative method\n",
    "                try:\n",
    "                    await self.page.evaluate('document.querySelector(\"input[name=l]\").value = \"\"')\n",
    "                    await self.page.locator('input[name=\"l\"]').fill(location)\n",
    "                except:\n",
    "                    print(\"Could not clear location field, continuing...\")\n",
    "\n",
    "            # Click search\n",
    "            await self.page.click('button[type=\"submit\"]')\n",
    "            await self.page.wait_for_load_state(\"domcontentloaded\", timeout=60000)\n",
    "            await asyncio.sleep(3)  # Wait for JavaScript to render results\n",
    "\n",
    "            # Take a screenshot for verification\n",
    "            screenshot_path = (\n",
    "                f\"search_results_{job_title.replace(' ', '_')}_{location.replace(' ', '_')}.png\"\n",
    "            )\n",
    "            await self.page.screenshot(path=screenshot_path)\n",
    "            print(f\"Saved search results screenshot to {screenshot_path}\")\n",
    "\n",
    "            # Process the specified number of pages\n",
    "            for page_num in range(max_pages):\n",
    "                try:\n",
    "                    print(f\"Processing page {page_num + 1} of {max_pages}...\")\n",
    "\n",
    "                    # If not on the first page, construct the URL for pagination\n",
    "                    if page_num > 0:\n",
    "                        current_url = await self.page.evaluate(\"window.location.href\")\n",
    "                        base_url = current_url.split(\"&start=\")[0]\n",
    "                        next_url = f\"{base_url}&start={page_num * 10}\"\n",
    "                        await self.page.goto(next_url, wait_until=\"domcontentloaded\")\n",
    "                        await asyncio.sleep(3)  # Wait for page to load\n",
    "\n",
    "                    # Extract job data from the current page\n",
    "                    print(\"Looking for job listings...\")\n",
    "                    page_jobs = await self.extract_jobs_from_page()\n",
    "\n",
    "                    if page_jobs:\n",
    "                        print(f\"Found {len(page_jobs)} jobs on page {page_num + 1}\")\n",
    "                        self.scraped_jobs.extend(page_jobs)\n",
    "                    else:\n",
    "                        print(f\"No jobs found on page {page_num + 1}\")\n",
    "                        break  # Exit if no jobs found (might be last page)\n",
    "\n",
    "                    # Random delay between pages\n",
    "                    if page_num < max_pages - 1:\n",
    "                        delay_time = random.uniform(3, 7)\n",
    "                        print(f\"Waiting {delay_time:.2f} seconds before next page...\")\n",
    "                        await asyncio.sleep(delay_time)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing page {page_num + 1}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "\n",
    "        print(f\"Total jobs scraped: {len(self.scraped_jobs)}\")\n",
    "        return self.scraped_jobs\n",
    "\n",
    "    async def extract_jobs_from_page(self):\n",
    "        \"\"\"Extract job information from the current page.\"\"\"\n",
    "        jobs = []\n",
    "\n",
    "        try:\n",
    "            # Let the page fully load\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            # Take a screenshot for debugging\n",
    "            await self.page.screenshot(path=\"page_loaded.png\")\n",
    "            print(\"Saved screenshot to page_loaded.png\")\n",
    "\n",
    "            # Try multiple selectors, from more specific to more general\n",
    "            selectors = [\n",
    "                \".cardOutline\",  # Modern Indeed card outline\n",
    "                'div[data-testid=\"jobListing\"]',  # Indeed job listings with testid\n",
    "                \"div.job_seen_beacon\",  # Job listing with beacon tracking\n",
    "                'div[class*=\"job_\"]',  # Any div with job_ in class name\n",
    "                \"div.resultContent\",  # Result content container\n",
    "                \"div.job-container\",  # Generic job container\n",
    "                \".jobsearch-ResultsList > div\",  # Any div in the results list\n",
    "            ]\n",
    "\n",
    "            for selector in selectors:\n",
    "                print(f\"Trying selector: {selector}\")\n",
    "                try:\n",
    "                    # Use a shorter timeout for each individual selector\n",
    "                    await self.page.wait_for_selector(selector, timeout=5000)\n",
    "                    job_cards = await self.page.query_selector_all(selector)\n",
    "\n",
    "                    if job_cards and len(job_cards) > 0:\n",
    "                        print(f\"Found {len(job_cards)} job cards with selector: {selector}\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Selector {selector} failed: {e}\")\n",
    "                    job_cards = []\n",
    "\n",
    "            # If visible browser mode, wait for manual intervention if needed\n",
    "            if not self.headless and not job_cards:\n",
    "                print(\"\\nNo job cards found automatically. The browser is visible.\")\n",
    "                print(\"You can manually check if the page has loaded correctly.\")\n",
    "                print(\"Press Enter in the notebook to continue...\")\n",
    "\n",
    "            if not job_cards or len(job_cards) == 0:\n",
    "                print(\"Could not find any job cards with standard selectors\")\n",
    "                # Last resort: try to find any divs that might contain jobs\n",
    "                try:\n",
    "                    # Get the page content and analyze structure\n",
    "                    page_html = await self.page.content()\n",
    "                    print(f\"Page content length: {len(page_html)} characters\")\n",
    "\n",
    "                    # Look for anything that might be a job listing\n",
    "                    job_cards = await self.page.query_selector_all('div[id*=\"job_\"]')\n",
    "                    if not job_cards:\n",
    "                        job_cards = await self.page.query_selector_all('div[class*=\"job\"]')\n",
    "                    if not job_cards:\n",
    "                        job_cards = await self.page.query_selector_all('div > a[id*=\"job\"]')\n",
    "                except Exception as e:\n",
    "                    print(f\"Last resort job finding failed: {e}\")\n",
    "\n",
    "            if not job_cards or len(job_cards) == 0:\n",
    "                print(\"No job cards found on page\")\n",
    "                return jobs\n",
    "\n",
    "            print(f\"Found {len(job_cards)} job cards on page\")\n",
    "\n",
    "            for i, card in enumerate(job_cards):\n",
    "                try:\n",
    "                    # Click the card to load job details in the panel\n",
    "                    await card.click()\n",
    "                    # Wait for job details to load\n",
    "                    await asyncio.sleep(1.5)\n",
    "\n",
    "                    job_data = await self.extract_single_job(card)\n",
    "                    if job_data:\n",
    "                        jobs.append(job_data)\n",
    "\n",
    "                    # Add some randomness to avoid detection\n",
    "                    await asyncio.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting job {i + 1}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding job cards: {str(e)}\")\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    async def extract_single_job(self, job_card):\n",
    "        \"\"\"\n",
    "        Extract detailed information from a single job card.\n",
    "\n",
    "        Args:\n",
    "            job_card: Playwright element representing a job posting\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with job information\n",
    "        \"\"\"\n",
    "        job_data = {\n",
    "            \"job_title\": None,\n",
    "            \"company_name\": None,\n",
    "            \"location\": None,\n",
    "            \"salary\": None,\n",
    "            \"job_description\": None,\n",
    "            \"scraped_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Extract job title - try multiple possible selectors\n",
    "            title_selectors = [\n",
    "                \"h2 a span\",\n",
    "                \"h2.jobTitle span\",\n",
    "                \"h2 span\",\n",
    "                \"h2\",\n",
    "                '[data-testid=\"jobTitle\"]',\n",
    "                \"a[data-jk] > span\",\n",
    "                \".jobTitle\",\n",
    "            ]\n",
    "\n",
    "            for selector in title_selectors:\n",
    "                title_element = await job_card.query_selector(selector)\n",
    "                if title_element:\n",
    "                    job_data[\"job_title\"] = (\n",
    "                        await title_element.get_attribute(\"title\")\n",
    "                        or await title_element.inner_text()\n",
    "                    )\n",
    "                    if job_data[\"job_title\"]:\n",
    "                        break\n",
    "\n",
    "            # Extract company name\n",
    "            company_selectors = [\n",
    "                '[data-testid=\"company-name\"]',\n",
    "                \"span.companyName\",\n",
    "                \".company_location > .companyName\",\n",
    "                '[data-testid=\"company-location\"] .companyName',\n",
    "                \".resultContent .company_location > div:first-child\",\n",
    "            ]\n",
    "\n",
    "            for selector in company_selectors:\n",
    "                company_element = await job_card.query_selector(selector)\n",
    "                if company_element:\n",
    "                    job_data[\"company_name\"] = await company_element.inner_text()\n",
    "                    if job_data[\"company_name\"]:\n",
    "                        break\n",
    "\n",
    "            # Extract location\n",
    "            location_selectors = [\n",
    "                '[data-testid=\"text-location\"]',\n",
    "                \"div.companyLocation\",\n",
    "                \".resultContent .metadataContainer .companyLocation\",\n",
    "                '[data-testid=\"company-location\"] .companyLocation',\n",
    "            ]\n",
    "\n",
    "            for selector in location_selectors:\n",
    "                location_element = await job_card.query_selector(selector)\n",
    "                if location_element:\n",
    "                    job_data[\"location\"] = await location_element.inner_text()\n",
    "                    if job_data[\"location\"]:\n",
    "                        break\n",
    "\n",
    "            # Extract salary information\n",
    "            # Method 1: Direct salary element\n",
    "            salary_selectors = [\n",
    "                \"span.salary-snippet\",\n",
    "                \"div.salary-snippet-container\",\n",
    "                'div[class*=\"salary\"]',\n",
    "                'span[class*=\"salary\"]',\n",
    "                \".metadata.salary-snippet-container\",\n",
    "                '.resultContent .metadataContainer [class*=\"salary\"]',\n",
    "            ]\n",
    "\n",
    "            for selector in salary_selectors:\n",
    "                salary_element = await job_card.query_selector(selector)\n",
    "                if salary_element:\n",
    "                    salary_text = await salary_element.inner_text()\n",
    "                    job_data[\"salary\"] = self.parse_salary(salary_text)\n",
    "                    if job_data[\"salary\"]:\n",
    "                        break\n",
    "\n",
    "            if not job_data[\"salary\"]:\n",
    "                # Method 2: Look in metadata\n",
    "                company_div = await job_card.query_selector('div[class*=\"company_location\"]')\n",
    "                if company_div:\n",
    "                    metadata_divs = await company_div.query_selector_all(\"+ div\")\n",
    "                    for div in metadata_divs:\n",
    "                        text = await div.inner_text()\n",
    "                        if any(\n",
    "                            keyword in text.lower()\n",
    "                            for keyword in [\"$\", \"hour\", \"year\", \"month\", \"annu\", \"sal\"]\n",
    "                        ):\n",
    "                            job_data[\"salary\"] = self.parse_salary(text)\n",
    "                            break\n",
    "\n",
    "            # Extract job description from the details panel (now visible since we clicked the card)\n",
    "            description_selectors = [\n",
    "                \"#jobDescriptionText\",\n",
    "                'div[id*=\"jobDescriptionText\"]',\n",
    "                'div[data-testid=\"jobDescriptionText\"]',\n",
    "                \"#jobDescriptionSection\",\n",
    "                \".jobsearch-jobDescriptionText\",\n",
    "            ]\n",
    "\n",
    "            for selector in description_selectors:\n",
    "                description_element = await self.page.query_selector(selector)\n",
    "                if description_element:\n",
    "                    job_data[\"job_description\"] = await description_element.inner_text()\n",
    "                    if job_data[\"job_description\"]:\n",
    "                        break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job details: {str(e)}\")\n",
    "\n",
    "        # Validate we got at least the essential data\n",
    "        if job_data[\"job_title\"] and job_data[\"company_name\"]:\n",
    "            return job_data\n",
    "        else:\n",
    "            print(\"Missing essential job data, skipping...\")\n",
    "            return None\n",
    "\n",
    "    def parse_salary(self, salary_text):\n",
    "        \"\"\"\n",
    "        Parse salary information from text.\n",
    "\n",
    "        Args:\n",
    "            salary_text: String containing salary information\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with salary range and period information\n",
    "        \"\"\"\n",
    "        if not salary_text:\n",
    "            return None\n",
    "\n",
    "        salary_data = {\"min\": None, \"max\": None, \"period\": None}\n",
    "\n",
    "        # Strip currency symbols and commas\n",
    "        text = salary_text.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "\n",
    "        # Try to detect period (hourly, yearly, etc.)\n",
    "        if \"hour\" in text.lower() or \"/hr\" in text.lower():\n",
    "            salary_data[\"period\"] = \"hourly\"\n",
    "        elif \"year\" in text.lower() or \"/yr\" in text.lower() or \"annual\" in text.lower():\n",
    "            salary_data[\"period\"] = \"yearly\"\n",
    "        elif \"month\" in text.lower() or \"/mo\" in text.lower():\n",
    "            salary_data[\"period\"] = \"monthly\"\n",
    "        elif \"week\" in text.lower() or \"/wk\" in text.lower():\n",
    "            salary_data[\"period\"] = \"weekly\"\n",
    "        elif \"day\" in text.lower() or \"/day\" in text.lower():\n",
    "            salary_data[\"period\"] = \"daily\"\n",
    "\n",
    "        # Try to extract salary range\n",
    "        # Look for patterns like $XX - $YY, $XX to $YY, $XX-$YY\n",
    "        # or just a single value like $XX\n",
    "        match = re.search(r\"(\\d+\\.?\\d*)\\s*(?:[-â€“â€”to]+)\\s*(\\d+\\.?\\d*)\", text)\n",
    "        if match:\n",
    "            salary_data[\"min\"] = float(match.group(1))\n",
    "            salary_data[\"max\"] = float(match.group(2))\n",
    "        else:\n",
    "            # Try to find a single number\n",
    "            match = re.search(r\"(\\d+\\.?\\d*)\", text)\n",
    "            if match:\n",
    "                value = float(match.group(1))\n",
    "                # If there's a single value, we don't know if it's min or max\n",
    "                # For simplicity, we'll set both to the same value\n",
    "                salary_data[\"min\"] = value\n",
    "                salary_data[\"max\"] = value\n",
    "\n",
    "        # If we couldn't extract any numbers, return None\n",
    "        if salary_data[\"min\"] is None and salary_data[\"max\"] is None:\n",
    "            return None\n",
    "\n",
    "        return salary_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Explain the importance of responsible scraping practices. Discuss rate limiting, robots.txt files, and ethical considerations. Emphasize that we're adding delays and using realistic user agents to be respectful to the website.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Search Functionality\n",
    "\n",
    "Now let's implement the core search functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method has been moved to the consolidated IndeedJobScraper class above\n",
    "# No need for separate definition here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Open the browser developer tools and show the class how to inspect elements to find the CSS selectors we'll use. This is a great hands-on moment to demonstrate how web scraping detective work happens.\n",
    "\n",
    "---\n",
    "\n",
    "## Extracting Job Details\n",
    "\n",
    "The core extraction logic handles the complexity of parsing individual job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These methods have been moved to the consolidated IndeedJobScraper class above\n",
    "# No need for separate definitions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Point out the defensive programming practices here - checking if elements exist before accessing them, handling exceptions gracefully. Ask the class: \"Why is this error handling so important in web scraping?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Salary Parsing and Data Cleaning\n",
    "\n",
    "One of the most challenging aspects is parsing salary information, which comes in various formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_salary(self, salary_text):\n",
    "    \"\"\"\n",
    "    Parse salary information from various formats.\n",
    "\n",
    "    Args:\n",
    "        salary_text: Raw salary text from job posting\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with parsed salary information\n",
    "    \"\"\"\n",
    "    if not salary_text:\n",
    "        return None\n",
    "\n",
    "    salary_info = {\n",
    "        \"raw_text\": salary_text,\n",
    "        \"min_annual\": None,\n",
    "        \"max_annual\": None,\n",
    "        \"currency\": \"USD\",\n",
    "        \"type\": None,  # 'hourly', 'annual', 'range'\n",
    "    }\n",
    "\n",
    "    # Clean the text\n",
    "    clean_text = re.sub(r\"[^\\d\\.\\-\\$,khourlyweekannualyear\\s]\", \"\", salary_text.lower())\n",
    "\n",
    "    # Extract numeric values\n",
    "    numbers = re.findall(r\"\\$?(\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)\", clean_text)\n",
    "\n",
    "    if not numbers:\n",
    "        return salary_info\n",
    "\n",
    "    # Convert strings to numbers\n",
    "    parsed_numbers = []\n",
    "    for num in numbers:\n",
    "        try:\n",
    "            parsed_numbers.append(float(num.replace(\",\", \"\")))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    if not parsed_numbers:\n",
    "        return salary_info\n",
    "\n",
    "    # Determine salary type and convert to annual\n",
    "    if \"hour\" in clean_text:\n",
    "        salary_info[\"type\"] = \"hourly\"\n",
    "        # Convert hourly to annual (assuming 40 hours/week, 52 weeks/year)\n",
    "        annual_multiplier = 40 * 52\n",
    "        salary_info[\"min_annual\"] = parsed_numbers[0] * annual_multiplier\n",
    "        if len(parsed_numbers) > 1:\n",
    "            salary_info[\"max_annual\"] = parsed_numbers[1] * annual_multiplier\n",
    "\n",
    "    elif (\n",
    "        \"year\" in clean_text or \"annual\" in clean_text or any(num > 1000 for num in parsed_numbers)\n",
    "    ):\n",
    "        salary_info[\"type\"] = \"annual\"\n",
    "        salary_info[\"min_annual\"] = parsed_numbers[0]\n",
    "        if len(parsed_numbers) > 1:\n",
    "            salary_info[\"max_annual\"] = parsed_numbers[1]\n",
    "\n",
    "    # Handle 'K' notation (e.g., \"50K - 70K\")\n",
    "    if \"k\" in clean_text:\n",
    "        salary_info[\"min_annual\"] = parsed_numbers[0] * 1000\n",
    "        if len(parsed_numbers) > 1:\n",
    "            salary_info[\"max_annual\"] = parsed_numbers[1] * 1000\n",
    "\n",
    "    return salary_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** This is a complex function that handles real-world data messiness. Walk through a few examples: \"$25/hour\", \"$50,000 - $70,000 per year\", \"Up to $80K annually\". Ask students to suggest other salary formats they've seen.\n",
    "\n",
    "---\n",
    "\n",
    "## Main Scraping Function\n",
    "\n",
    "Let's put it all together with our main scraping function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_indeed_jobs(job_title, location, max_pages=3, headless=False):\n",
    "    \"\"\"\n",
    "    Main function to scrape Indeed jobs.\n",
    "\n",
    "    Args:\n",
    "        job_title: Job title to search for\n",
    "        location: Location to search in\n",
    "        max_pages: Maximum pages to scrape\n",
    "        headless: Whether to run browser in headless mode (default: False for better results with Indeed)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with scraped job data\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Starting scraper for '{job_title}' in '{location}'...\")\n",
    "    print(f\"Browser will run in {'headless' if headless else 'visible'} mode\")\n",
    "\n",
    "    try:\n",
    "        # Use context manager for browser lifecycle management\n",
    "        async with IndeedJobScraper(headless=headless) as scraper:\n",
    "            # Perform the search and extract jobs\n",
    "            jobs = await scraper.search_jobs(job_title, location, max_pages)\n",
    "\n",
    "            if not jobs:\n",
    "                print(\"No jobs found. Website structure might have changed or no results matched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Convert to DataFrame for analysis\n",
    "            df = pd.DataFrame(jobs)\n",
    "\n",
    "            if not df.empty:\n",
    "                # Clean and enhance the data\n",
    "                df = clean_scraped_data(df)\n",
    "\n",
    "                # Save to CSV\n",
    "                filename = f\"data/indeed_jobs_{job_title.replace(' ', '_')}_{location.replace(' ', '_')}.csv\"\n",
    "                df.to_csv(filename, index=False)\n",
    "                print(f\"Data saved to: {filename}\")\n",
    "\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during scraping: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def clean_scraped_data(df):\n",
    "    \"\"\"\n",
    "    Clean and enhance the scraped job data.\n",
    "\n",
    "    Args:\n",
    "        df: Raw DataFrame from scraper\n",
    "\n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Remove duplicates based on job title and company\n",
    "    df = df.drop_duplicates(subset=[\"job_title\", \"company_name\"], keep=\"first\")\n",
    "\n",
    "    # Clean text fields\n",
    "    text_columns = [\"job_title\", \"company_name\", \"location\", \"job_description\"]\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            df[col] = df[col].replace(\"nan\", None)\n",
    "            df[col] = df[col].replace(\"None\", None)\n",
    "\n",
    "    # Extract salary ranges into separate columns\n",
    "    if \"salary\" in df.columns and df[\"salary\"].notna().any():\n",
    "        # Handle cases where salary might be a string representation of a dict\n",
    "        for i, salary in enumerate(df[\"salary\"]):\n",
    "            if isinstance(salary, str) and salary.startswith(\"{\"):\n",
    "                try:\n",
    "                    df.at[i, \"salary\"] = json.loads(salary)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Now normalize the salary data\n",
    "        try:\n",
    "            salary_df = pd.json_normalize(df[\"salary\"].dropna())\n",
    "            for col in [\"min\", \"max\", \"period\"]:\n",
    "                if col in salary_df.columns:\n",
    "                    df[f\"salary_{col}\"] = salary_df[col]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing salary data: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Emphasize the importance of data cleaning in any data pipeline. Ask: \"What other cleaning steps might we want to add? How could we validate the quality of our scraped data?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Executing the Scraper\n",
    "\n",
    "Now let's use our scraper to collect data for the target occupations we identified in Chapter 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our target occupations from Chapter 1\n",
    "try:\n",
    "    target_occupations = pd.read_csv(\"data/bls_jobs_metro_area.csv\")\n",
    "    # Rename columns to match expected names in following cells\n",
    "    target_occupations[\"occupation\"] = target_occupations[\"OCC_TITLE\"]\n",
    "    target_occupations[\"state\"] = target_occupations[\"PRIM_STATE\"]\n",
    "    print(\"Target occupations for scraping:\")\n",
    "    print(target_occupations[[\"occupation\", \"state\"]].to_string(index=False))\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please ensure 'data/bls_jobs_metro_area.csv' exists.\")\n",
    "    # Create empty dataframe with expected structure for testing\n",
    "    target_occupations = pd.DataFrame({\"occupation\": [], \"state\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_all_targets():\n",
    "    \"\"\"Scrape job data for all our target occupations.\"\"\"\n",
    "    all_scraped_data = []\n",
    "\n",
    "    # Check if target_occupations is empty or doesn't have the expected columns\n",
    "    if target_occupations.empty:\n",
    "        print(\"No target occupations found to scrape.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Make sure the required columns exist\n",
    "    required_cols = [\"occupation\", \"state\"]\n",
    "    if not all(col in target_occupations.columns for col in required_cols):\n",
    "        print(f\"Missing required columns in target_occupations. Needed: {required_cols}\")\n",
    "        print(f\"Available: {target_occupations.columns.tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Limit to first 3 targets for demo purposes\n",
    "    target_subset = target_occupations.head(3)\n",
    "    print(f\"Will scrape {len(target_subset)} targets (limiting for workshop purposes)\")\n",
    "    print(\"Using non-headless browser mode for better results with Indeed\")\n",
    "\n",
    "    for i, row in target_subset.iterrows():\n",
    "        job_title = row[\"occupation\"]\n",
    "        state = row[\"state\"]\n",
    "\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Scraping: {job_title} in {state}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        try:\n",
    "            # Scrape jobs for this target\n",
    "            df = await scrape_indeed_jobs(\n",
    "                job_title=job_title,\n",
    "                location=state,\n",
    "                max_pages=2,  # Limit for demo purposes\n",
    "                headless=False,  # Use visible browser for better results\n",
    "            )\n",
    "\n",
    "            if not df.empty:\n",
    "                # Add metadata\n",
    "                df[\"target_occupation\"] = job_title\n",
    "                df[\"target_state\"] = state\n",
    "                all_scraped_data.append(df)\n",
    "\n",
    "                print(f\"Successfully scraped {len(df)} jobs\")\n",
    "            else:\n",
    "                print(\"No jobs found for this target\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {job_title} in {state}: {str(e)}\")\n",
    "\n",
    "        # Be respectful to the website\n",
    "        await asyncio.sleep(random.uniform(5, 10))\n",
    "\n",
    "    # Combine all scraped data\n",
    "    if all_scraped_data:\n",
    "        try:\n",
    "            combined_df = pd.concat(all_scraped_data, ignore_index=True)\n",
    "            combined_df.to_csv(\"data/indeed_jobs_combined.csv\", index=False)\n",
    "            print(f\"\\nTotal jobs scraped: {len(combined_df)}\")\n",
    "            print(\"Combined data saved to: data/indeed_jobs_combined.csv\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining scraped data: {e}\")\n",
    "            if all_scraped_data and len(all_scraped_data) > 0:\n",
    "                print(\"Returning first dataset as fallback\")\n",
    "                return all_scraped_data[0]\n",
    "\n",
    "    print(\"No data was successfully scraped\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Execute the scraping\n",
    "print(\"Ready to start scraping. Uncomment the next line to begin.\")\n",
    "scraped_jobs_df = await scrape_all_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Run this code live, but be prepared for potential issues (rate limiting, website changes, etc.). Use any problems as teaching moments about the challenges of web scraping. If Indeed blocks requests, switch to a smaller demo or use pre-scraped sample data.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Quality Assessment\n",
    "\n",
    "Let's examine the quality and structure of our scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze our scraped data\n",
    "if not scraped_jobs_df.empty:\n",
    "    print(\"=== SCRAPED DATA ANALYSIS ===\")\n",
    "    print(f\"Total jobs scraped: {len(scraped_jobs_df)}\")\n",
    "    print(f\"Unique companies: {scraped_jobs_df['company_name'].nunique()}\")\n",
    "    print(f\"Jobs with salary info: {scraped_jobs_df['salary'].notna().sum()}\")\n",
    "    print(\n",
    "        f\"Average job description length: {scraped_jobs_df['job_description'].str.len().mean():.0f} characters\"\n",
    "    )\n",
    "\n",
    "    # Show sample of the data\n",
    "    print(\"\\nSample of scraped jobs:\")\n",
    "    display_columns = [\"job_title\", \"company_name\", \"location\", \"salary_min_annual\"]\n",
    "    print(scraped_jobs_df[display_columns].head().to_string(index=False))\n",
    "\n",
    "    # Salary analysis\n",
    "    salary_data = scraped_jobs_df[scraped_jobs_df[\"salary_min_annual\"].notna()]\n",
    "    if not salary_data.empty:\n",
    "        print(\"\\nSalary Statistics:\")\n",
    "        print(f\"Average minimum salary: ${salary_data['salary_min_annual'].mean():,.0f}\")\n",
    "        print(\n",
    "            f\"Salary range: ${salary_data['salary_min_annual'].min():,.0f} - ${salary_data['salary_min_annual'].max():,.0f}\"\n",
    "        )\n",
    "\n",
    "    # Data completeness analysis\n",
    "    print(\"\\nData Completeness:\")\n",
    "    for col in [\"job_title\", \"company_name\", \"location\", \"salary\", \"job_description\"]:\n",
    "        if col in scraped_jobs_df.columns:\n",
    "            completeness = (scraped_jobs_df[col].notna().sum() / len(scraped_jobs_df)) * 100\n",
    "            print(f\"{col}: {completeness:.1f}% complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scraper on a single job title\n",
    "async def test_scraper():\n",
    "    \"\"\"Test our scraper on a single job and location.\"\"\"\n",
    "    print(\"Starting test scraper...\")\n",
    "\n",
    "    # Create simple test target\n",
    "    my_target = {\"job_title\": \"Data Scientist\", \"location\": \"Oregon\"}\n",
    "\n",
    "    try:\n",
    "        # Create the scraper - with visible browser window\n",
    "        scraper = IndeedJobScraper(headless=False)  # Non-headless mode for better results\n",
    "\n",
    "        # Start the browser\n",
    "        await scraper.start_browser()\n",
    "\n",
    "        try:\n",
    "            # Search for jobs\n",
    "            print(f\"Searching for {my_target['job_title']} in {my_target['location']}...\")\n",
    "            jobs = await scraper.search_jobs(\n",
    "                job_title=my_target[\"job_title\"],\n",
    "                location=my_target[\"location\"],\n",
    "                max_pages=1,  # Just 1 page for testing\n",
    "            )\n",
    "\n",
    "            # Check results\n",
    "            if jobs:\n",
    "                print(f\"Success! Found {len(jobs)} job postings.\")\n",
    "                # Convert to DataFrame for easier viewing\n",
    "                test_job_results = pd.DataFrame(jobs)\n",
    "                # Show a summary\n",
    "                print(\"\\nFirst job posting:\")\n",
    "                for key, value in jobs[0].items():\n",
    "                    if key != \"job_description\":  # Description is too long to print\n",
    "                        print(f\"{key}: {value}\")\n",
    "\n",
    "                print(\"\\nSummary of all jobs:\")\n",
    "                print(test_job_results[[\"job_title\", \"company_name\", \"location\"]].head())\n",
    "\n",
    "                return test_job_results\n",
    "            else:\n",
    "                print(\"No jobs found in test search.\")\n",
    "                print(\"This could be because:\")\n",
    "                print(\"1. The selectors need updating for Indeed's current layout\")\n",
    "                print(\"2. Indeed detected automation and showed a CAPTCHA\")\n",
    "                print(\"3. There are no matching job listings in the selected location\")\n",
    "\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        finally:\n",
    "            print(\"\\nScraping test complete. Closing browser.\")\n",
    "            # Always close the browser when done\n",
    "            await scraper.close_browser()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in test scraper: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Run the test\n",
    "print(\"Starting scraper test with non-headless browser...\")\n",
    "test_job_results = await test_scraper()\n",
    "print(\"Test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback for workshop scenarios if scraping fails\n",
    "def load_fallback_data():\n",
    "    \"\"\"\n",
    "    Load pre-scraped data as a fallback for workshop scenarios\n",
    "    where scraping might fail due to rate limiting or website changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Attempting to load pre-scraped fallback data...\")\n",
    "        # Try to load from a fallback file if it exists\n",
    "        fallback_file = Path(\"data/indeed_jobs_combined.csv\")\n",
    "\n",
    "        if fallback_file.exists():\n",
    "            fallback_df = pd.read_csv(fallback_file)\n",
    "            print(f\"Loaded {len(fallback_df)} pre-scraped job records\")\n",
    "            return fallback_df\n",
    "        else:\n",
    "            # If no fallback file exists, create synthetic data\n",
    "            print(\"No pre-scraped data found. Creating synthetic data for workshop purposes...\")\n",
    "\n",
    "            # Create sample data for a few jobs\n",
    "            sample_data = []\n",
    "            job_titles = [\"Data Scientist\", \"Software Developer\", \"Machine Learning Engineer\"]\n",
    "            companies = [\"TechCorp\", \"DataInnovations\", \"AI Solutions\", \"CodeMasters\"]\n",
    "            locations = [\"Portland, OR\", \"Seattle, WA\", \"San Francisco, CA\"]\n",
    "\n",
    "            for i in range(20):  # Generate 20 sample jobs\n",
    "                min_salary = random.randint(80000, 150000)\n",
    "                max_salary = random.randint(150000, 200000)\n",
    "\n",
    "                job = {\n",
    "                    \"job_title\": random.choice(job_titles),\n",
    "                    \"company_name\": random.choice(companies),\n",
    "                    \"location\": random.choice(locations),\n",
    "                    \"salary\": json.dumps(\n",
    "                        {\n",
    "                            \"raw_text\": f\"${min_salary // 1000}k - ${max_salary // 1000}k per year\",\n",
    "                            \"min_annual\": min_salary,\n",
    "                            \"max_annual\": max_salary,\n",
    "                            \"currency\": \"USD\",\n",
    "                            \"type\": \"annual\",\n",
    "                        }\n",
    "                    ),\n",
    "                    \"salary_min_annual\": min_salary,\n",
    "                    \"salary_max_annual\": max_salary,\n",
    "                    \"salary_type\": \"annual\",\n",
    "                    \"job_description\": \"This is a sample job description for workshop purposes.\",\n",
    "                    \"scraped_at\": datetime.now().isoformat(),\n",
    "                    \"target_occupation\": job_titles[0],\n",
    "                    \"target_state\": \"OR\",\n",
    "                }\n",
    "                sample_data.append(job)\n",
    "\n",
    "            fallback_df = pd.DataFrame(sample_data)\n",
    "\n",
    "            # Save the synthetic data for future use\n",
    "            fallback_df.to_csv(\"data/indeed_jobs_combined.csv\", index=False)\n",
    "            print(f\"Created {len(fallback_df)} synthetic job records\")\n",
    "            return fallback_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fallback data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# If scraping wasn't successful, use fallback data\n",
    "if (\n",
    "    \"scraped_jobs_df\" not in locals()\n",
    "    or isinstance(scraped_jobs_df, pd.DataFrame)\n",
    "    and scraped_jobs_df.empty\n",
    "):\n",
    "    print(\"No scraped data available. Using fallback data instead.\")\n",
    "    scraped_jobs_df = load_fallback_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Use this analysis to discuss data quality issues common in web scraping. Point out missing values, inconsistent formats, and potential data validation needs. Ask: \"What patterns do you notice? What might cause missing salary information?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing Data for Module 2\n",
    "\n",
    "Finally, let's prepare our scraped data for the next phase of analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_analysis(df):\n",
    "    \"\"\"\n",
    "    Prepare scraped data for machine learning analysis in Module 2.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with scraped job data\n",
    "\n",
    "    Returns:\n",
    "        Cleaned DataFrame ready for analysis\n",
    "    \"\"\"\n",
    "    # Create a working copy\n",
    "    analysis_df = df.copy()\n",
    "\n",
    "    # Feature engineering\n",
    "    if \"job_description\" in analysis_df.columns:\n",
    "        # Add text length features\n",
    "        analysis_df[\"description_length\"] = analysis_df[\"job_description\"].str.len()\n",
    "        analysis_df[\"description_word_count\"] = analysis_df[\"job_description\"].str.split().str.len()\n",
    "\n",
    "    # Create salary availability indicator\n",
    "    analysis_df[\"has_salary_info\"] = analysis_df[\"salary\"].notna()\n",
    "\n",
    "    # Extract state from location for consistency\n",
    "    if \"location\" in analysis_df.columns:\n",
    "        analysis_df[\"state_extracted\"] = analysis_df[\"location\"].str.extract(r\", ([A-Z]{2})\")[0]\n",
    "\n",
    "    # Create company size indicators (based on name patterns)\n",
    "    if \"company_name\" in analysis_df.columns:\n",
    "        known_large_companies = [\"Google\", \"Microsoft\", \"Amazon\", \"Apple\", \"Meta\", \"Netflix\"]\n",
    "        analysis_df[\"is_big_tech\"] = analysis_df[\"company_name\"].str.contains(\n",
    "            \"|\".join(known_large_companies), case=False, na=False\n",
    "        )\n",
    "\n",
    "    return analysis_df\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "if not scraped_jobs_df.empty:\n",
    "    analysis_ready_df = prepare_for_analysis(scraped_jobs_df)\n",
    "\n",
    "    # Save the analysis-ready dataset\n",
    "    analysis_ready_df.to_csv(\"data/jobs_for_analysis.csv\", index=False)\n",
    "    print(\"Analysis-ready dataset saved to: data/jobs_for_analysis.csv\")\n",
    "\n",
    "    # Show what we've prepared\n",
    "    print(\"\\nDataset prepared for Module 2:\")\n",
    "    print(f\"- {len(analysis_ready_df)} job postings\")\n",
    "    print(f\"- {analysis_ready_df.columns.tolist()}\")\n",
    "    print(\"- Ready for regression modeling and AI analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handling Common Scraping Challenges\n",
    "\n",
    "> **Instructor Cue:** This is an important teachable moment. Discuss real-world scraping challenges and solutions. If students encountered errors during the demo, use those as examples.\n",
    "\n",
    "### Challenge 1: Rate Limiting and IP Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScraper(IndeedJobScraper):\n",
    "    \"\"\"Enhanced scraper with better error handling and rate limiting.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.request_count = 0\n",
    "        self.max_requests_per_hour = 100\n",
    "\n",
    "    async def handle_rate_limiting(self):\n",
    "        \"\"\"Handle rate limiting gracefully.\"\"\"\n",
    "        self.request_count += 1\n",
    "\n",
    "        if self.request_count % 10 == 0:  # Every 10 requests\n",
    "            print(f\"Processed {self.request_count} requests. Taking a longer break...\")\n",
    "            await asyncio.sleep(random.uniform(10, 20))\n",
    "        else:\n",
    "            await asyncio.sleep(random.uniform(2, 5))\n",
    "\n",
    "    async def retry_on_failure(self, operation, max_retries=3):\n",
    "        \"\"\"Retry operations that might fail due to network issues.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return await operation()\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}. Retrying...\")\n",
    "                await asyncio.sleep(random.uniform(5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Challenge 2: Dynamic Content and JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def wait_for_dynamic_content(self, selector, timeout=10000):\n",
    "    \"\"\"Wait for dynamically loaded content.\"\"\"\n",
    "    try:\n",
    "        await self.page.wait_for_selector(selector, timeout=timeout)\n",
    "        # Additional wait for any animations or lazy loading\n",
    "        await self.page.wait_for_timeout(1000)\n",
    "    except:\n",
    "        print(f\"Timeout waiting for selector: {selector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Challenge 3: Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_scraped_job(job_data):\n",
    "    \"\"\"\n",
    "    Validate that a scraped job has minimum required information.\n",
    "\n",
    "    Args:\n",
    "        job_data: Dictionary with job information\n",
    "\n",
    "    Returns:\n",
    "        Boolean indicating if job data is valid\n",
    "    \"\"\"\n",
    "    required_fields = [\"job_title\", \"company_name\"]\n",
    "\n",
    "    # Check required fields exist and are not empty\n",
    "    for field in required_fields:\n",
    "        if not job_data.get(field) or str(job_data[field]).strip() == \"\":\n",
    "            return False\n",
    "\n",
    "    # Additional validation rules\n",
    "    if job_data.get(\"job_title\") and len(job_data[\"job_title\"]) > 200:\n",
    "        return False  # Suspiciously long title\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module 1 Summary and Transition\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "> **Instructor Cue:** Take a moment to celebrate what the class has accomplished. This is a significant amount of technical work, and they should feel proud of building a complete data collection pipeline.\n",
    "\n",
    "In Module 1, we've built a complete data discovery and collection pipeline:\n",
    "\n",
    "1. **Exploratory Data Analysis**: Used government data to understand the job market landscape\n",
    "2. **Target Identification**: Applied data-driven decision making to select specific occupations and locations\n",
    "3. **Web Scraping Implementation**: Built a robust, production-ready scraper using Playwright\n",
    "4. **Data Quality Assurance**: Implemented validation and cleaning procedures\n",
    "5. **Pipeline Integration**: Connected our analysis results to our scraping targets\n",
    "\n",
    "### Data Assets Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize our data assets\n",
    "print(\"=== MODULE 1 DATA ASSETS ===\")\n",
    "print(\"âœ“ OEWS national dataset (loaded and analyzed)\")\n",
    "print(\"âœ“ Target occupations CSV (data/bls_jobs_metro_area.csv)\")\n",
    "print(\"âœ“ Scraped job postings (data/indeed_jobs_combined.csv)\")\n",
    "print(\"âœ“ Analysis-ready dataset (data/jobs_for_analysis.csv)\")\n",
    "print(\"\\nReady for Module 2: Machine Learning and Regression Analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Looking Ahead to Module 2\n",
    "\n",
    "> **Instructor Cue:** Build excitement for the next module. Ask: \"Now that we have both government data and real-time job market data, what questions could we answer? What predictions might we make?\"\n",
    "\n",
    "With our rich dataset combining government statistics and current job postings, Module 2 will focus on:\n",
    "\n",
    "- **Regression Modeling**: Predicting salaries based on job characteristics\n",
    "- **Feature Engineering**: Extracting insights from job descriptions using NLP\n",
    "- **Model Interpretation**: Understanding what factors drive compensation\n",
    "- **Data Visualization**: Creating compelling stories with our combined datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-On Exercise: Test Your Own Target\n",
    "\n",
    "> **Instructor Cue:** This is the main exercise for Chapter 2. Students will use the targets they identified in Chapter 1 or choose from the examples below. Give them 10-15 minutes to run their searches and analyze results.\n",
    "\n",
    "**Exercise: Scrape Your Chosen Occupation**\n",
    "\n",
    "Using the job scraper we built, test it with your target from Chapter 1 or choose one from these interesting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example targets from our BLS data (choose one or use your own from Chapter 1)\n",
    "\n",
    "# Let's generate examples based on our actual BLS data\n",
    "example_targets = []\n",
    "\n",
    "if \"target_occupations\" in locals() and not target_occupations.empty:\n",
    "    # Use actual data from our CSV\n",
    "    for i, row in target_occupations.head(4).iterrows():\n",
    "        example_targets.append({\"job_title\": row[\"occupation\"], \"location\": row[\"state\"]})\n",
    "else:\n",
    "    # Fallback examples if data isn't available\n",
    "    example_targets = [\n",
    "        {\"job_title\": \"Data Scientists\", \"location\": \"OR\"},\n",
    "        {\"job_title\": \"Software Developers\", \"location\": \"CA\"},\n",
    "        {\"job_title\": \"Computer Systems Analysts\", \"location\": \"WA\"},\n",
    "        {\"job_title\": \"Information Security Analysts\", \"location\": \"NY\"},\n",
    "    ]\n",
    "\n",
    "# Display available examples\n",
    "print(\"Available targets for scraping:\")\n",
    "for i, target in enumerate(example_targets):\n",
    "    print(f\"{i + 1}. {target['job_title']} in {target['location']}\")\n",
    "\n",
    "# Choose your target\n",
    "my_target = example_targets[0]  # Change index or define your own\n",
    "\n",
    "# Test the scraper with your chosen target\n",
    "print(f\"\\nTesting scraper for: {my_target['job_title']} in {my_target['location']}\")\n",
    "\n",
    "\n",
    "# Run a small test (1 page only for time)\n",
    "async def test_scraper():\n",
    "    print(\"Starting test scraper...\")\n",
    "    test_results = await scrape_indeed_jobs(\n",
    "        job_title=my_target[\"job_title\"],\n",
    "        location=my_target[\"location\"],\n",
    "        max_pages=1,  # Keep it quick for the workshop\n",
    "        headless=True,\n",
    "    )\n",
    "\n",
    "    # Quick analysis of your results\n",
    "    if not test_results.empty:\n",
    "        print(f\"\\nâœ… Successfully scraped {len(test_results)} jobs!\")\n",
    "        print(f\"Companies found: {test_results['company_name'].nunique()}\")\n",
    "\n",
    "        # Check if salary columns exist\n",
    "        if \"salary\" in test_results.columns:\n",
    "            print(f\"Jobs with salary info: {test_results['salary'].notna().sum()}\")\n",
    "\n",
    "        # Show a sample\n",
    "        print(\"\\nSample results:\")\n",
    "        sample_cols = [\n",
    "            col for col in [\"job_title\", \"company_name\", \"location\"] if col in test_results.columns\n",
    "        ]\n",
    "        print(test_results[sample_cols].head(3).to_string(index=False))\n",
    "\n",
    "        return test_results\n",
    "    else:\n",
    "        print(\"âš ï¸ No results found. Try a different job title or location.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Uncomment to run the test\n",
    "print(\"Ready to test scraper. Running the test...\")\n",
    "test_job_results = await test_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Walk around and help students troubleshoot. Common issues: network problems, Indeed rate limiting, or job titles that are too specific. Have backup pre-scraped data ready if needed.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Module 1*\n",
    "\n",
    "> **Instructor Cue:** End with a strong transition and celebrate their accomplishment: \"You've just built a sophisticated data collection system that companies pay thousands of dollars for. You've gone from government statistics to real-time job market data in two hours! In the next module, we'll use this rich dataset to build predictive models and create AI-powered insights. Let's take a 15-minute break before diving into machine learning!\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,md",
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "nsbe-pdc2025-python-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
