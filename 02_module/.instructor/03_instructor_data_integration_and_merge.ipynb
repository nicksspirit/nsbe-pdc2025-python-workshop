{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chapter 3: Complete Data Integration\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Create AI-powered location matching between different data formats\n",
    "- Integrate all cleaning functions to process scraped data end-to-end\n",
    "- Merge Indeed and OEWS datasets using common keys\n",
    "- Produce a final clean dataset ready for dashboard creation\n",
    "\n",
    "---\n",
    "\n",
    "## Bringing It All Together\n",
    "\n",
    "> **Instructor Cue:** Start with energy and urgency: \"We're in the home stretch! We have salary parsing and job title matching. Now we need one more piece - location matching - and then we can merge our datasets for the final time before lunch.\"\n",
    "\n",
    "We've built powerful AI tools for salary parsing and job title matching. Now let's complete the puzzle with location matching and create our final integrated dataset.\n",
    "\n",
    "### The Location Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick setup and data loading\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.google import GoogleModel\n",
    "from pydantic_ai.providers.google import GoogleProvider\n",
    "import nest_asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility and load environment\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Google AI provider\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "provider = GoogleProvider(api_key=API_KEY)\n",
    "model = GoogleModel(\"gemini-1.5-flash\", provider=provider)\n",
    "\n",
    "# Load OEWS data\n",
    "bls_file = Path(\"../01_module/data/bls_jobs_metro_area_2024.csv\")\n",
    "oews_df = pd.read_csv(bls_file)\n",
    "\n",
    "# Load scraped Indeed data using consistent pattern\n",
    "scraped_file_paths = list(Path(\"../01_module/data\").glob(\"scraped_indeed_*.csv\"))\n",
    "dataframes = [pd.read_csv(file) for file in scraped_file_paths]\n",
    "indeed_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(scraped_file_paths)} scraped files with {len(indeed_df)} total jobs\")\n",
    "\n",
    "print(\"\\n=== LOCATION MATCHING CHALLENGE ===\")\n",
    "print(\"Indeed locations (sample):\")\n",
    "for location in indeed_df[\"location\"].unique()[:5]:\n",
    "    print(f\"  â€¢ {location}\")\n",
    "\n",
    "print(\"\\nOEWS AREA_TITLE (sample):\")\n",
    "for area in oews_df[\"AREA_TITLE\"].unique()[:5]:\n",
    "    print(f\"  â€¢ {area}\")\n",
    "\n",
    "print(\"\\nOEWS PRIM_STATE (sample):\")\n",
    "print(f\"  States: {sorted(oews_df['PRIM_STATE'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Point out that Indeed has \"Portland, OR\" while OEWS has \"Portland-Vancouver-Hillsboro, OR-WA\" for AREA_TITLE and \"OR\" for PRIM_STATE. Ask: \"How would we match these automatically?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Creating Location Matching Function\n",
    "\n",
    "Let's build our final AI function to match locations between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for a SINGLE location match. Added 'original_location' for easier mapping.\n",
    "class LocationMatch(BaseModel):\n",
    "    original_location: str = Field(\n",
    "        ..., description=\"The original location string from the input list.\"\n",
    "    )\n",
    "    matched_area: str = Field(\n",
    "        ..., description=\"Best matching AREA_TITLE from the provided OEWS options.\"\n",
    "    )\n",
    "    matched_state: str = Field(\n",
    "        ..., description=\"Best matching PRIM_STATE from the provided OEWS options.\"\n",
    "    )\n",
    "    confidence: int = Field(\n",
    "        ..., description=\"A 1-10 confidence score for the overall match quality.\"\n",
    "    )\n",
    "    reasoning: str = Field(..., description=\"A brief explanation for the match decision.\")\n",
    "\n",
    "\n",
    "# Model for a BATCH of location matches\n",
    "class LocationMatchResults(BaseModel):\n",
    "    matches: list[LocationMatch]\n",
    "\n",
    "\n",
    "# The Agent is created once and configured for the batch location matching task.\n",
    "# This avoids the inefficiency of creating a new agent on every function call.\n",
    "_location_matcher = Agent(\n",
    "    \"gemini-1.5-flash\",  # Assumes 'model' is a pre-configured GoogleModel instance\n",
    "    output_type=LocationMatchResults,\n",
    "    system_prompt=\"\"\"\n",
    "    You are an expert at matching a list of informal geographic locations to official US government area and state names.\n",
    "\n",
    "    Your task: For each input location, find the best matching official OEWS metropolitan statistical area (AREA_TITLE) and primary state (PRIM_STATE) from the provided lists.\n",
    "\n",
    "    Guidelines:\n",
    "    1. Match each location from the input list to the best available AREA_TITLE and PRIM_STATE.\n",
    "    2. AREA_TITLE is a metropolitan area (e.g., \"Portland-Vancouver-Hillsboro, OR-WA\").\n",
    "    3. PRIM_STATE is a 2-letter state code (e.g., \"OR\", \"TX\", \"CA\").\n",
    "    4. If no reasonable area match is found, use \"Unknown\" for matched_area.\n",
    "    5. Always attempt to match the state, even if the area is unknown.\n",
    "    6. For general terms like \"Remote\" or \"United States\", both area and state should be \"Unknown\".\n",
    "    7. Provide a confidence score (1-10) and a brief reasoning for each match.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def ai_match_location2msa(\n",
    "    indeed_locations: list[str], oews_areas: list[str], oews_states: list[str]\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses a single AI batch call to match a list of Indeed locations to OEWS areas and states.\n",
    "\n",
    "    Args:\n",
    "        indeed_locations: A list of location strings from Indeed.\n",
    "        oews_areas: A complete list of AREA_TITLE values from OEWS.\n",
    "        oews_states: A complete list of PRIM_STATE values from OEWS.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing the match result for a location.\n",
    "    \"\"\"\n",
    "    # Filter out any empty or null locations before processing\n",
    "    valid_locations = [loc for loc in indeed_locations if pd.notna(loc) and str(loc).strip()]\n",
    "    if not valid_locations:\n",
    "        return []\n",
    "\n",
    "    # Create the context and the list of items to match for the prompt\n",
    "    area_options = \"\\n\".join([f\"- {area}\" for area in oews_areas])\n",
    "    state_options = \"\\n\".join([f\"- {state}\" for state in sorted(oews_states)])\n",
    "    locations_to_match = \"\\n\".join([f'- \"{loc}\"' for loc in valid_locations])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Please process the entire list of \"Locations to Match\" below. For each one, find the best fit from the provided official options.\n",
    "\n",
    "    Official AREA_TITLE Options:\n",
    "    {area_options}\n",
    "    - Unknown\n",
    "\n",
    "    Official PRIM_STATE Options:\n",
    "    {state_options}\n",
    "    - Unknown\n",
    "\n",
    "    Locations to Match:\n",
    "    {locations_to_match}\n",
    "\n",
    "    Return a structured list containing a match object for every location in the \"Locations to Match\" list.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = _location_matcher.run_sync(prompt)\n",
    "        return [match.model_dump() for match in result.output.matches]\n",
    "    except Exception as e:\n",
    "        print(f\"A critical AI error occurred during location matching: {e}\")\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"original_location\": loc,\n",
    "                \"matched_area\": \"Error\",\n",
    "                \"matched_state\": \"Error\",\n",
    "                \"confidence\": 0,\n",
    "                \"reasoning\": f\"AI processing failed: {str(e)}\",\n",
    "            }\n",
    "            for loc in valid_locations\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing Location Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_areas = oews_df[\"AREA_TITLE\"].unique().tolist()\n",
    "unique_states = oews_df[\"PRIM_STATE\"].unique().tolist()\n",
    "\n",
    "# A list of test locations to match.\n",
    "test_locations = [\n",
    "    \"Portland, OR\",\n",
    "    \"New Orleans, LA\",\n",
    "    \"Austin, TX\",\n",
    "    \"Bend, OR\",\n",
    "    \"Remote Work\",\n",
    "    \"San Francisco Bay Area\",\n",
    "    \"USA\",\n",
    "]\n",
    "\n",
    "print(\"=== TESTING BATCH LOCATION MATCHING ===\")\n",
    "print(f\"{'Indeed Location':<30} | {'Matched Area':<45} | {'State':<6} | {'Conf':<4} | Reasoning\")\n",
    "print(\"-\" * 125)\n",
    "\n",
    "# Call the batch function once with the entire list of locations.\n",
    "all_matched_results = ai_match_location2msa(test_locations, unique_areas, unique_states)\n",
    "\n",
    "# --- 3. Loop Through the Results and Print ---\n",
    "# Iterate through the list of result dictionaries to display each match.\n",
    "for result in all_matched_results:\n",
    "    # Use .get() for safe access to dictionary keys, providing defaults.\n",
    "    original = result.get(\"original_location\", \"N/A\")\n",
    "    area = result.get(\"matched_area\", \"N/A\")\n",
    "    state = result.get(\"matched_state\", \"N/A\")\n",
    "    confidence = result.get(\"confidence\", 0)\n",
    "    reasoning = result.get(\"reasoning\", \"N/A\")\n",
    "\n",
    "    # Use string slicing on the area name to keep the table neatly aligned.\n",
    "    print(f\"{original:<30} | {area[:44]:<45} | {state:<6} | {confidence:<4} | {reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "> **Instructor Cue:** Watch the AI work! Point out successful matches and discuss any interesting decisions. This completes our AI toolkit.\n",
    "\n",
    "---\n",
    "\n",
    "## Adding Location Function to Janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../workshoplib/src/workshoplib/janitor.py\n",
    "\n",
    "# Model for a SINGLE location match. Added 'original_location' for easier mapping.\n",
    "class LocationMatch(BaseModel):\n",
    "    original_location: str = Field(\n",
    "        ..., description=\"The original location string from the input list.\"\n",
    "    )\n",
    "    matched_area: str = Field(\n",
    "        ..., description=\"Best matching AREA_TITLE from the provided OEWS options.\"\n",
    "    )\n",
    "    matched_state: str = Field(\n",
    "        ..., description=\"Best matching PRIM_STATE from the provided OEWS options.\"\n",
    "    )\n",
    "    confidence: int = Field(\n",
    "        ..., description=\"A 1-10 confidence score for the overall match quality.\"\n",
    "    )\n",
    "    reasoning: str = Field(..., description=\"A brief explanation for the match decision.\")\n",
    "\n",
    "\n",
    "# Model for a BATCH of location matches\n",
    "class LocationMatchResults(BaseModel):\n",
    "    matches: list[LocationMatch]\n",
    "\n",
    "\n",
    "# --- 2. Create a Reusable, Global AI Agent ---\n",
    "\n",
    "# The Agent is created once and configured for the batch location matching task.\n",
    "# This avoids the inefficiency of creating a new agent on every function call.\n",
    "_location_matcher = Agent(\n",
    "    \"gemini-1.5-flash\",  # Assumes 'model' is a pre-configured GoogleModel instance\n",
    "    output_type=LocationMatchResults,\n",
    "    system_prompt=\"\"\"\n",
    "    You are an expert at matching a list of informal geographic locations to official US government area and state names.\n",
    "\n",
    "    Your task: For each input location, find the best matching official OEWS metropolitan statistical area (AREA_TITLE) and primary state (PRIM_STATE) from the provided lists.\n",
    "\n",
    "    Guidelines:\n",
    "    1. Match each location from the input list to the best available AREA_TITLE and PRIM_STATE.\n",
    "    2. AREA_TITLE is a metropolitan area (e.g., \"Portland-Vancouver-Hillsboro, OR-WA\").\n",
    "    3. PRIM_STATE is a 2-letter state code (e.g., \"OR\", \"TX\", \"CA\").\n",
    "    4. If no reasonable area match is found, use \"Unknown\" for matched_area.\n",
    "    5. Always attempt to match the state, even if the area is unknown.\n",
    "    6. For general terms like \"Remote\" or \"United States\", both area and state should be \"Unknown\".\n",
    "    7. Provide a confidence score (1-10) and a brief reasoning for each match.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def ai_match_location2msa(\n",
    "    indeed_locations: list[str], oews_areas: list[str], oews_states: list[str]\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses a single AI batch call to match a list of Indeed locations to OEWS areas and states.\n",
    "\n",
    "    Args:\n",
    "        indeed_locations: A list of location strings from Indeed.\n",
    "        oews_areas: A complete list of AREA_TITLE values from OEWS.\n",
    "        oews_states: A complete list of PRIM_STATE values from OEWS.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing the match result for a location.\n",
    "    \"\"\"\n",
    "    # Filter out any empty or null locations before processing\n",
    "    valid_locations = [loc for loc in indeed_locations if pd.notna(loc) and str(loc).strip()]\n",
    "    if not valid_locations:\n",
    "        return []\n",
    "\n",
    "    # Create the context and the list of items to match for the prompt\n",
    "    area_options = \"\\n\".join([f\"- {area}\" for area in oews_areas])\n",
    "    state_options = \"\\n\".join([f\"- {state}\" for state in sorted(oews_states)])\n",
    "    locations_to_match = \"\\n\".join([f'- \"{loc}\"' for loc in valid_locations])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Please process the entire list of \"Locations to Match\" below. For each one, find the best fit from the provided official options.\n",
    "\n",
    "    Official AREA_TITLE Options:\n",
    "    {area_options}\n",
    "    - Unknown\n",
    "\n",
    "    Official PRIM_STATE Options:\n",
    "    {state_options}\n",
    "    - Unknown\n",
    "\n",
    "    Locations to Match:\n",
    "    {locations_to_match}\n",
    "\n",
    "    Return a structured list containing a match object for every location in the \"Locations to Match\" list.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = _location_matcher.run_sync(prompt)\n",
    "        return [match.model_dump() for match in result.output.matches]\n",
    "    except Exception as e:\n",
    "        print(f\"A critical AI error occurred during location matching: {e}\")\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"original_location\": loc,\n",
    "                \"matched_area\": \"Error\",\n",
    "                \"matched_state\": \"Error\",\n",
    "                \"confidence\": 0,\n",
    "                \"reasoning\": f\"AI processing failed: {str(e)}\",\n",
    "            }\n",
    "            for loc in valid_locations\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables (like your API key) from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Apply a patch to allow nested asyncio event loops, which is helpful in notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Update function names to match the latest batch-processing versions\n",
    "from workshoplib.janitor import ai_parse_salaries, ai_match_job_titles, ai_match_location2msa\n",
    "\n",
    "# Update the printed list of available functions to be accurate\n",
    "print(\"âœ… Complete janitor module ready!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"  â€¢ traditional_parse_salaries (rule-based batch processing)\")\n",
    "print(\"  â€¢ ai_parse_salaries (AI-powered batch processing)\")\n",
    "print(\"  â€¢ ai_match_job_titles (AI-powered batch job title matching)\")\n",
    "print(\"  â€¢ ai_match_locations_to_msa_batch (AI-powered batch location matching)\")\n",
    "\n",
    "# --- Update the test section to use the batch function ---\n",
    "# Assuming 'oews_df' is a pre-loaded pandas DataFrame\n",
    "unique_areas = oews_df[\"AREA_TITLE\"].unique()\n",
    "unique_states = oews_df[\"PRIM_STATE\"].unique()\n",
    "test_locations = [\"Portland, OR\", \"San Jose, CA\", \"Remote\"]\n",
    "\n",
    "print(f\"\\nTesting batch location matching with: {test_locations}\")\n",
    "\n",
    "# Call the batch function ONCE, which is more efficient\n",
    "results = ai_match_location2msa(test_locations, unique_areas, unique_states)\n",
    "\n",
    "# Loop through the returned list of result dictionaries to display them\n",
    "for result in results:\n",
    "    original_loc = result.get(\"original_location\", \"N/A\")\n",
    "    matched_area = result.get(\"matched_area\", \"N/A\")\n",
    "    matched_state = result.get(\"matched_state\", \"N/A\")\n",
    "    confidence = result.get(\"confidence\", 0)\n",
    "\n",
    "    print(f\"  '{original_loc}' â†’ '{matched_area}', {matched_state} (confidence: {confidence}/10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Data Cleaning Pipeline\n",
    "\n",
    "Now let's use all our functions together to clean our scraped Indeed data.\n",
    "\n",
    "> **Instructor Cue:** This is the payoff moment! Say: \"Watch how all our work comes together. We're about to clean messy scraped data in just a few lines of code using our AI toolkit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean our Indeed data using all our janitor functions\n",
    "def clean_indeed_data(indeed_df, oews_df):\n",
    "    \"\"\"\n",
    "    Apply all our cleaning functions to Indeed data using efficient batch processing.\n",
    "\n",
    "    Args:\n",
    "        indeed_df: Raw scraped Indeed data\n",
    "        oews_df: OEWS reference data\n",
    "\n",
    "    Returns:\n",
    "        Cleaned Indeed DataFrame ready for merging\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§¹ Starting complete data cleaning pipeline...\")\n",
    "\n",
    "    # Start with a copy\n",
    "    clean_df = indeed_df.copy()\n",
    "\n",
    "    # 1. Clean salary data using AI batch processing\n",
    "    print(\"  Step 1: AI batch salary parsing...\")\n",
    "    salary_list = clean_df[\"salary\"].dropna().tolist()\n",
    "    if salary_list:\n",
    "        salary_results = ai_parse_salaries(salary_list)\n",
    "\n",
    "        # Create mapping from salary string to result\n",
    "        salary_mapping = {}\n",
    "        for i, salary in enumerate(salary_list):\n",
    "            if i < len(salary_results):\n",
    "                salary_mapping[salary] = salary_results[i]\n",
    "\n",
    "        # Apply to dataframe\n",
    "        for idx, row in clean_df.iterrows():\n",
    "            if pd.notna(row[\"salary\"]) and row[\"salary\"] in salary_mapping:\n",
    "                result = salary_mapping[row[\"salary\"]]\n",
    "                clean_df.at[idx, \"min_salary\"] = result.get(\"min_salary\")\n",
    "                clean_df.at[idx, \"max_salary\"] = result.get(\"max_salary\")\n",
    "                clean_df.at[idx, \"salary_type\"] = result.get(\"salary_type\")\n",
    "                clean_df.at[idx, \"salary_confidence\"] = result.get(\"confidence\")\n",
    "\n",
    "    # 2. Match job titles using AI batch processing\n",
    "    print(\"  Step 2: AI batch job title matching...\")\n",
    "    target_titles = oews_df[\"OCC_TITLE\"].unique().tolist()\n",
    "\n",
    "    # Get unique titles and process in batch\n",
    "    unique_titles = clean_df[\"job_title\"].dropna().unique().tolist()\n",
    "    if unique_titles:\n",
    "        title_results = ai_match_job_titles(unique_titles, target_titles)\n",
    "\n",
    "        # Create mapping from original title to result\n",
    "        title_mapping = {}\n",
    "        for result in title_results:\n",
    "            original = result.get(\"original_title\")\n",
    "            if original:\n",
    "                title_mapping[original] = result\n",
    "\n",
    "        # Apply to dataframe\n",
    "        for idx, row in clean_df.iterrows():\n",
    "            if pd.notna(row[\"job_title\"]) and row[\"job_title\"] in title_mapping:\n",
    "                match_data = title_mapping[row[\"job_title\"]]\n",
    "                clean_df.at[idx, \"bls_title\"] = match_data.get(\"matched_title\")\n",
    "                clean_df.at[idx, \"title_confidence\"] = match_data.get(\"confidence\")\n",
    "                clean_df.at[idx, \"title_reasoning\"] = match_data.get(\"reasoning\")\n",
    "\n",
    "    # 3. Match locations using AI (individual calls for now - could be batched later)\n",
    "    print(\"  Step 3: AI location matching...\")\n",
    "    unique_areas = oews_df[\"AREA_TITLE\"].unique()\n",
    "    unique_states = oews_df[\"PRIM_STATE\"].unique()\n",
    "\n",
    "    unique_locations = clean_df[\"location\"].dropna().unique().tolist()\n",
    "\n",
    "    # Process all locations in a single batch call\n",
    "    location_results = ai_match_location2msa(unique_locations, unique_areas, unique_states)\n",
    "\n",
    "    # Create mapping from original location to result\n",
    "    location_mapping = {}\n",
    "    for result in location_results:\n",
    "        original = result.get(\"original_location\")\n",
    "        if original:\n",
    "            location_mapping[original] = result\n",
    "\n",
    "    # Apply to dataframe\n",
    "    for idx, row in clean_df.iterrows():\n",
    "        if pd.notna(row[\"location\"]) and row[\"location\"] in location_mapping:\n",
    "            match_data = location_mapping[row[\"location\"]]\n",
    "            clean_df.at[idx, \"matched_area\"] = match_data.get(\"matched_area\")\n",
    "            clean_df.at[idx, \"matched_state\"] = match_data.get(\"matched_state\")\n",
    "            clean_df.at[idx, \"location_confidence\"] = match_data.get(\"confidence\")\n",
    "\n",
    "    print(\"âœ… Data cleaning complete!\")\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "# Clean our data\n",
    "indeed_clean = clean_indeed_data(indeed_df, oews_df)\n",
    "\n",
    "print(\"\\n=== CLEANING RESULTS ===\")\n",
    "print(f\"Jobs processed: {len(indeed_clean)}\")\n",
    "print(f\"Jobs with salary data: {indeed_clean['min_salary'].notna().sum()}\")\n",
    "print(f\"Jobs with BLS title matches: {indeed_clean['bls_title'].notna().sum()}\")\n",
    "print(f\"Jobs with location matches: {indeed_clean['matched_area'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Dataset Merge\n",
    "\n",
    "Now for the moment we've been building toward - merging Indeed and OEWS data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Indeed and OEWS data\n",
    "def merge_datasets(indeed_clean, oews_df):\n",
    "    \"\"\"\n",
    "    Merge cleaned Indeed data with OEWS official statistics.\n",
    "\n",
    "    Args:\n",
    "        indeed_clean: Cleaned Indeed DataFrame\n",
    "        oews_df: OEWS reference data\n",
    "\n",
    "    Returns:\n",
    "        Merged DataFrame with both datasets\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”— Merging Indeed and OEWS datasets...\")\n",
    "\n",
    "    # Prepare OEWS data for merging\n",
    "    oews_merge = oews_df.copy()\n",
    "    oews_merge = oews_merge.rename(columns={\"OCC_TITLE\": \"bls_title\"})\n",
    "\n",
    "    # Select key OEWS columns for the final dataset\n",
    "    keep_columns = [\n",
    "        \"bls_title\",\n",
    "        \"AREA_TITLE\",\n",
    "        \"PRIM_STATE\",\n",
    "        \"A_MEAN\",\n",
    "        \"H_MEAN\",\n",
    "        \"TOT_EMP\",\n",
    "        \"JOBS_1000\",\n",
    "        \"H_PCT25\",\n",
    "        \"H_MEDIAN\",\n",
    "        \"H_PCT75\",\n",
    "        \"A_PCT25\",\n",
    "        \"A_MEDIAN\",\n",
    "        \"A_PCT75\",\n",
    "    ]\n",
    "\n",
    "    oews_merge = oews_merge[keep_columns]\n",
    "\n",
    "    # Merge on job title and location\n",
    "    merged = pd.merge(\n",
    "        indeed_clean,\n",
    "        oews_merge,\n",
    "        left_on=[\"bls_title\", \"matched_area\", \"matched_state\"],\n",
    "        right_on=[\"bls_title\", \"AREA_TITLE\", \"PRIM_STATE\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    merged.columns = merged.columns.str.lower()\n",
    "\n",
    "    print(f\"  Indeed jobs before merge: {len(indeed_clean)}\")\n",
    "    print(f\"  OEWS records: {len(oews_df)}\")\n",
    "    print(f\"  Successfully merged: {len(merged)} records\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Execute the merge\n",
    "final_dataset = merge_datasets(indeed_clean, oews_df)\n",
    "\n",
    "print(\"\\n=== FINAL DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {final_dataset.shape}\")\n",
    "print(\"Final columns:\")\n",
    "import pprint as pp\n",
    "\n",
    "pp.pprint(list(final_dataset.columns), compact=True)\n",
    "\n",
    "# Show sample of final data\n",
    "print(\"\\nSample of merged data:\")\n",
    "\n",
    "display(final_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Final Dataset\n",
    "\n",
    "Let's save our completed dataset for the afternoon dashboard module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final clean dataset\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "final_path = DATA_DIR / \"indeed_jobs_cleaned.csv\"\n",
    "final_dataset.to_csv(final_path, index=False)\n",
    "\n",
    "print(f\"ðŸ’¾ Final dataset saved: {final_path}\")\n",
    "print(f\"ðŸ“Š Records: {len(final_dataset)}\")\n",
    "print(f\"ðŸ“‹ Columns: {len(final_dataset.columns)}\")\n",
    "\n",
    "# Create a quick summary\n",
    "if len(final_dataset) > 0:\n",
    "    summary = {\n",
    "        \"total_jobs\": len(final_dataset),\n",
    "        \"unique_companies\": final_dataset[\"company_name\"].nunique(),\n",
    "        \"unique_bls_titles\": final_dataset[\"bls_title\"].nunique(),\n",
    "        \"unique_areas\": final_dataset[\"matched_area\"].nunique(),\n",
    "        \"jobs_with_salary\": final_dataset[\"min_salary\"].notna().sum(),\n",
    "        \"average_confidence\": {\n",
    "            \"salary\": final_dataset[\"salary_confidence\"].mean(),\n",
    "            \"title\": final_dataset[\"title_confidence\"].mean(),\n",
    "            \"location\": final_dataset[\"location_confidence\"].mean(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== FINAL SUMMARY ===\")\n",
    "    print(f\"ðŸ“ˆ Total jobs in final dataset: {summary['total_jobs']}\")\n",
    "    print(f\"ðŸ¢ Unique companies: {summary['unique_companies']}\")\n",
    "    print(f\"ðŸ’¼ BLS job categories: {summary['unique_bls_titles']}\")\n",
    "    print(f\"ðŸ“ Geographic areas: {summary['unique_areas']}\")\n",
    "    print(f\"ðŸ’° Jobs with salary data: {summary['jobs_with_salary']}\")\n",
    "    print(\"ðŸ¤– Average AI confidence scores:\")\n",
    "    print(f\"   Salary parsing: {summary['average_confidence']['salary']:.1f}/10\")\n",
    "    print(f\"   Title matching: {summary['average_confidence']['title']:.1f}/10\")\n",
    "    print(f\"   Location matching: {summary['average_confidence']['location']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module 2 Complete: Ready for Lunch!\n",
    "\n",
    "> **Instructor Cue:** End with celebration and excitement for the afternoon: \"Look what we've accomplished! We've taken messy scraped data and used AI to create a professional-grade integrated dataset. After lunch, we'll turn this into an beautiful interactive dashboard!\"\n",
    "\n",
    "### ðŸŽ‰ What We've Accomplished\n",
    "\n",
    "**Module 2 Complete Success:**\n",
    "- âœ… **Salary Parsing** - Traditional AND AI-powered approaches\n",
    "- âœ… **Job Title Matching** - Intelligent mapping between datasets  \n",
    "- âœ… **Location Matching** - Geographic standardization with AI\n",
    "- âœ… **Complete Integration** - Successfully merged Indeed + OEWS data\n",
    "- âœ… **Reusable Tools** - Built a complete janitor module for future use\n",
    "\n",
    "**Technical Achievements:**\n",
    "- Built 4 sophisticated data cleaning functions\n",
    "- Processed real-world messy data automatically\n",
    "- Created AI agents that outperform traditional rule-based approaches\n",
    "- Integrated multiple data sources with common keys\n",
    "- Produced a clean, analysis-ready dataset\n",
    "\n",
    "**Key Insights Learned:**\n",
    "- AI excels at handling data complexity and variations\n",
    "- Structured outputs make AI reliable for data processing\n",
    "- Confidence scores help identify data quality issues\n",
    "- Modular functions enable rapid development and reuse\n",
    "\n",
    "### ðŸ½ï¸ Lunch Break - See You at 1:00 PM!\n",
    "\n",
    "> **Instructor Cue:** Send them off with energy: \"You've done the hard work of data integration. After lunch, we get to the fun part - building an interactive dashboard that showcases all these insights. Rest up, because this afternoon we're going to create something amazing!\"\n",
    "\n",
    "**This Afternoon: Module 3 Preview**\n",
    "- Build an interactive Streamlit dashboard\n",
    "- Create dynamic visualizations of our integrated data\n",
    "- Add AI-powered features to the dashboard  \n",
    "- Generate PDF reports\n",
    "- Deploy our complete data application\n",
    "\n",
    "**What to Bring Back:**\n",
    "- Your laptop (obviously!)\n",
    "- Energy for building something visual and interactive\n",
    "- Questions about the data insights you want to explore\n",
    "\n",
    "---\n",
    "\n",
    "*End of Module 2 - Enjoy your lunch! ðŸŒ®*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,md",
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec,-jupytext.text_representation.jupytext_version"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
